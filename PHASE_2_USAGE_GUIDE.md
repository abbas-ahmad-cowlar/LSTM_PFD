# Phase 2: 1D CNN Pipeline - Usage Guide

This guide explains how to use the Phase 2 CNN implementation for bearing fault diagnosis using deep learning on raw vibration signals.

---

## ðŸ“‹ What Was Implemented

Phase 2 adds **24 new files** implementing the complete 1D CNN pipeline:

### Core CNN Architecture (8 files - Tier 0)
- **models/cnn/**: CNN architectures and building blocks
  - `conv_blocks.py`: Modular convolutional blocks (standard, residual, separable)
  - `cnn_1d.py`: Baseline 1D CNN (~1.2M parameters, 5 layers)
  - `attention_cnn.py`: Attention-based CNN with SE blocks and temporal attention
  - `multi_scale_cnn.py`: Multi-scale CNN with Inception modules and dilated convolutions

### Data Pipeline (3 files - Tier 0)
- **data/**: Signal preprocessing and loading
  - `cnn_transforms.py`: Signal preprocessing (normalize, augment, to_tensor)
  - `cnn_dataset.py`: RawSignalDataset for loading signals without feature extraction
  - `cnn_dataloader.py`: Optimized DataLoaders with pin_memory

### Training Infrastructure (3 files - Tier 0)
- **training/**: Training components
  - `cnn_losses.py`: Label smoothing, focal loss, contrastive loss
  - `cnn_optimizer.py`: AdamW, SGD configurations
  - `cnn_trainer.py`: Mixed precision training with gradient clipping

### Advanced Training (3 files - Tier 1)
- **training/**: Advanced training features
  - `cnn_schedulers.py`: Advanced LR scheduling (cosine, one-cycle, warmup)
  - `cnn_callbacks.py`: Training monitoring callbacks
- **data/**:
  - `signal_augmentation.py`: Advanced augmentations (mixup, time warping)

### Evaluation & Experiments (2 files - Tier 1)
- **evaluation/**: Model evaluation
  - `cnn_evaluator.py`: Complete evaluation suite
- **experiments/**:
  - `cnn_experiment.py`: End-to-end experiment orchestration

### Utilities (2 files - Tier 1)
- **utils/**: Supporting utilities
  - `checkpoint_manager.py`: Model checkpointing with top-k tracking
  - `early_stopping.py`: Early stopping with warmup support

### Visualization & Analysis (2 files - Tier 3)
- **visualization/**: CNN-specific visualization
  - `cnn_visualizer.py`: Visualize filters, feature maps, activations
  - `cnn_analysis.py`: Gradient flow, saliency maps, failure analysis

### Scripts (3 files - Tier 3)
- **scripts/**: Command-line tools
  - `train_cnn.py`: Training script
  - `evaluate_cnn.py`: Evaluation script
  - `inference_cnn.py`: Inference/demo script

**Target Performance**: 93-96% test accuracy (match Phase 1 classical ML baseline)

---

## ðŸŽ¯ Available CNN Architectures

Phase 2 provides 5 CNN architectures with different trade-offs:

| Model | Parameters | Expected Accuracy | Use Case |
|-------|-----------|------------------|----------|
| `CNN1D` | ~1.2M | 93-95% | Baseline, fast training |
| `AttentionCNN1D` | ~1.5M | 94-96% | Best accuracy, attention mechanisms |
| `LightweightAttentionCNN` | ~500K | 92-94% | Resource-constrained, edge deployment |
| `MultiScaleCNN1D` | ~1.3M | 94-96% | Multi-scale features, Inception-style |
| `DilatedMultiScaleCNN` | ~1.0M | 93-95% | Dilated convs, expanded receptive field |

---

## ðŸš€ Quick Start

### Step 1: Install Dependencies

```bash
# Core dependencies (if not already installed)
pip install torch torchvision numpy scipy scikit-learn matplotlib seaborn

# Optional (for advanced features)
pip install tensorboard mlflow
```

### Step 2: Train Your First CNN

```bash
# Train baseline CNN (quick)
python scripts/train_cnn.py \
    --model cnn1d \
    --epochs 50 \
    --batch-size 32 \
    --lr 0.001

# Train attention CNN (best accuracy)
python scripts/train_cnn.py \
    --model attention \
    --epochs 100 \
    --batch-size 32 \
    --lr 0.001 \
    --mixed-precision \
    --early-stopping
```

Training typically takes:
- **CPU**: 30-60 minutes per epoch (not recommended)
- **GPU (NVIDIA RTX 3090)**: 2-3 minutes per epoch
- **Full training (50 epochs)**: ~2 hours on GPU

### Step 3: Evaluate the Trained Model

```bash
# Basic evaluation
python scripts/evaluate_cnn.py \
    --checkpoint checkpoints/cnn1d/model_best.pth

# Comprehensive evaluation with plots
python scripts/evaluate_cnn.py \
    --checkpoint checkpoints/attention/model_best.pth \
    --plot-confusion \
    --plot-roc \
    --per-class-metrics \
    --analyze-failures
```

### Step 4: Run Inference

```bash
# Single signal prediction
python scripts/inference_cnn.py \
    --checkpoint checkpoints/cnn1d/model_best.pth \
    --signal-file test_signal.npy \
    --verbose

# Interactive demo mode
python scripts/inference_cnn.py \
    --checkpoint checkpoints/cnn1d/model_best.pth \
    --demo
```

---

## ðŸ“š Detailed Usage Examples

### Example 1: End-to-End Training Pipeline

```python
"""
train_custom_cnn.py - Custom CNN training with Python API
"""
import torch
from pathlib import Path

from models.cnn.cnn_1d import CNN1D
from data.cnn_dataloader import create_cnn_dataloaders
from training.cnn_trainer import CNNTrainer
from training.cnn_optimizer import create_optimizer
from training.cnn_losses import create_loss_function
from data.signal_generator import SignalGenerator
from config.data_config import DataConfig

# 1. Generate data
config = DataConfig(num_signals_per_fault=150, rng_seed=42)
generator = SignalGenerator(config)
dataset = generator.generate_dataset()

signals = dataset['signals']
labels = dataset['labels']

# 2. Create dataloaders
train_loader, val_loader, test_loader = create_cnn_dataloaders(
    signals=signals,
    labels=labels,
    batch_size=32,
    train_ratio=0.7,
    val_ratio=0.15,
    test_ratio=0.15,
    seed=42,
    augment_train=True
)

print(f"Train: {len(train_loader.dataset)} samples")
print(f"Val:   {len(val_loader.dataset)} samples")
print(f"Test:  {len(test_loader.dataset)} samples")

# 3. Create model
model = CNN1D(num_classes=11, input_length=102400, dropout=0.3)
print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

# 4. Create optimizer and loss
optimizer = create_optimizer(
    model.parameters(),
    optimizer_name='adamw',
    lr=0.001,
    weight_decay=0.0001
)

criterion = create_loss_function(
    loss_name='label_smoothing',
    num_classes=11,
    label_smoothing=0.1
)

# 5. Create trainer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
trainer = CNNTrainer(
    model=model,
    optimizer=optimizer,
    criterion=criterion,
    device=device,
    mixed_precision=True,
    grad_clip=1.0
)

# 6. Training loop
num_epochs = 50
best_val_acc = 0.0

for epoch in range(num_epochs):
    print(f"\nEpoch {epoch+1}/{num_epochs}")

    # Train
    train_metrics = trainer.train_epoch(train_loader, epoch)
    print(f"Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.4f}")

    # Validate
    val_metrics = trainer.validate(val_loader)
    print(f"Val   - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}")

    # Save best model
    if val_metrics['accuracy'] > best_val_acc:
        best_val_acc = val_metrics['accuracy']
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': best_val_acc
        }, 'best_model.pth')
        print(f"âœ“ Saved best model (val_acc={best_val_acc:.4f})")

# 7. Test evaluation
test_metrics = trainer.validate(test_loader)
print(f"\nTest - Loss: {test_metrics['loss']:.4f}, Acc: {test_metrics['accuracy']:.4f}")
```

### Example 2: Visualize CNN Filters and Activations

```python
"""
visualize_cnn.py - Visualize CNN internals
"""
import torch
from models.cnn.cnn_1d import CNN1D
from visualization.cnn_visualizer import CNNVisualizer

# Load trained model
model = CNN1D(num_classes=11, input_length=102400)
checkpoint = torch.load('checkpoints/cnn1d/model_best.pth')
model.load_state_dict(checkpoint['model_state_dict'])

# Create visualizer
visualizer = CNNVisualizer(model)

# 1. Visualize convolutional filters
visualizer.plot_conv_filters(
    save_path='figures/filters.png'
)

# 2. Visualize feature maps for a test signal
test_signal = torch.randn(1, 1, 102400)
visualizer.plot_feature_maps(
    test_signal,
    layer_name='conv1',
    save_path='figures/feature_maps_conv1.png'
)

# 3. Analyze activation distributions
visualizer.plot_activation_distributions(
    test_signal,
    save_path='figures/activation_distributions.png'
)

# 4. Calculate receptive fields
receptive_fields = visualizer.plot_receptive_field(
    save_path='figures/receptive_fields.png'
)

print("Receptive fields per layer:")
for layer, rf_info in receptive_fields.items():
    print(f"  {layer}: RF={rf_info['receptive_field']}, stride={rf_info['stride']}")
```

### Example 3: Analyze Model Behavior

```python
"""
analyze_cnn.py - Deep analysis of CNN model
"""
import torch
from models.cnn.attention_cnn import AttentionCNN1D
from visualization.cnn_analysis import CNNAnalyzer
from data.cnn_dataloader import create_cnn_dataloaders

# Load model
model = AttentionCNN1D(num_classes=11, input_length=102400)
checkpoint = torch.load('checkpoints/attention/model_best.pth')
model.load_state_dict(checkpoint['model_state_dict'])

# Create analyzer
analyzer = CNNAnalyzer(model)

# 1. Analyze gradient flow (detect vanishing/exploding gradients)
# Load training data
train_loader, _, _ = create_cnn_dataloaders(...)

gradient_stats = analyzer.analyze_gradient_flow(
    train_loader,
    num_batches=10,
    save_path='figures/gradient_flow.png'
)

# 2. Compute saliency map (which parts of signal are important)
test_signal = torch.randn(1, 1, 102400, requires_grad=True)
saliency = analyzer.compute_saliency_map(
    test_signal,
    target_class=3,
    save_path='figures/saliency_map.png'
)

# 3. Occlusion sensitivity (robust feature importance)
sensitivity = analyzer.occlusion_sensitivity(
    test_signal,
    target_class=3,
    window_size=1024,
    stride=512,
    save_path='figures/occlusion_sensitivity.png'
)

# 4. Analyze failure cases (misclassifications)
_, _, test_loader = create_cnn_dataloaders(...)

failure_analysis = analyzer.analyze_failure_cases(
    test_loader,
    class_names=CLASS_NAMES,
    n_cases=20,
    save_path='figures/failure_cases.png'
)

print(f"Most confused pairs:")
for (true_idx, pred_idx), count in failure_analysis['confusion_pairs'].items():
    print(f"  {CLASS_NAMES[true_idx]} â†’ {CLASS_NAMES[pred_idx]}: {count} cases")
```

### Example 4: Compare Multiple Architectures

```python
"""
compare_models.py - Compare different CNN architectures
"""
import torch
from models.cnn.cnn_1d import CNN1D
from models.cnn.attention_cnn import AttentionCNN1D
from models.cnn.multi_scale_cnn import MultiScaleCNN1D
from evaluation.cnn_evaluator import CNNEvaluator

models = {
    'CNN1D': CNN1D(num_classes=11, input_length=102400),
    'AttentionCNN': AttentionCNN1D(num_classes=11, input_length=102400),
    'MultiScaleCNN': MultiScaleCNN1D(num_classes=11, input_length=102400)
}

# Load checkpoints
for name, model in models.items():
    checkpoint = torch.load(f'checkpoints/{name}/model_best.pth')
    model.load_state_dict(checkpoint['model_state_dict'])

# Evaluate all models
results = {}
for name, model in models.items():
    evaluator = CNNEvaluator(model)
    results[name] = evaluator.evaluate(test_loader)

# Compare
print("\n" + "="*80)
print("Model Comparison")
print("="*80)
print(f"{'Model':<20} {'Accuracy':<12} {'F1-Score':<12} {'Parameters':<15}")
print("-"*80)

for name, res in results.items():
    params = sum(p.numel() for p in models[name].parameters())
    print(f"{name:<20} {res['accuracy']:<12.4f} {res['macro_f1']:<12.4f} {params:<15,}")

print("="*80)
```

---

## ðŸŽ¨ Visualization Gallery

Phase 2 includes powerful visualization tools:

### 1. Filter Visualization
```python
from visualization.cnn_visualizer import CNNVisualizer

visualizer = CNNVisualizer(model)
visualizer.plot_conv_filters(save_path='filters.png')
```

Shows learned convolutional filters as 1D signals.

### 2. Feature Maps
```python
visualizer.plot_feature_maps(
    signal, layer_name='conv3',
    save_path='feature_maps.png'
)
```

Visualizes intermediate activations at each layer.

### 3. Activation Distributions
```python
visualizer.plot_activation_distributions(
    signal,
    save_path='activations.png'
)
```

Detects dead neurons and activation saturation.

### 4. Saliency Maps
```python
from visualization.cnn_analysis import CNNAnalyzer

analyzer = CNNAnalyzer(model)
saliency = analyzer.compute_saliency_map(
    signal, target_class=5,
    save_path='saliency.png'
)
```

Shows which parts of the input signal are most important for prediction.

---

## ðŸ”§ Advanced Features

### Mixed Precision Training

Speeds up training by 2-3x with minimal accuracy loss:

```bash
python scripts/train_cnn.py \
    --model attention \
    --mixed-precision \
    --batch-size 64  # Can use larger batch size with FP16
```

### Learning Rate Scheduling

Multiple scheduler options:

```bash
# Cosine annealing
python scripts/train_cnn.py --scheduler cosine

# One-cycle policy (fast convergence)
python scripts/train_cnn.py --scheduler onecycle

# Warmup + cosine
python scripts/train_cnn.py --scheduler warmup_cosine --warmup-epochs 5
```

### Advanced Loss Functions

```bash
# Focal loss (for class imbalance)
python scripts/train_cnn.py --loss focal

# Label smoothing (regularization)
python scripts/train_cnn.py --loss label_smoothing --label-smoothing 0.1
```

### Data Augmentation

Automatic augmentation during training:

```python
from data.signal_augmentation import SignalAugmenter

augmenter = SignalAugmenter(
    time_shift_prob=0.5,
    magnitude_scale_prob=0.5,
    mixup_prob=0.3,
    time_warp_prob=0.2
)

augmented_signal = augmenter(signal)
```

---

## ðŸ“Š Expected Results

### Baseline CNN (CNN1D)
- **Test Accuracy**: 93-95%
- **Training Time**: ~2 hours (50 epochs, GPU)
- **Inference**: ~5ms per signal
- **Parameters**: ~1.2M

### Attention CNN (AttentionCNN1D)
- **Test Accuracy**: 94-96%
- **Training Time**: ~3 hours (100 epochs, GPU)
- **Inference**: ~8ms per signal
- **Parameters**: ~1.5M
- **Improvement**: +1-2% over baseline

### Multi-Scale CNN (MultiScaleCNN1D)
- **Test Accuracy**: 94-96%
- **Training Time**: ~2.5 hours (50 epochs, GPU)
- **Inference**: ~7ms per signal
- **Parameters**: ~1.3M

---

## ðŸ› Troubleshooting

### Issue: Out of Memory (CUDA OOM)

**Solution**: Reduce batch size or use gradient accumulation

```bash
# Reduce batch size
python scripts/train_cnn.py --batch-size 16

# Or use gradient accumulation (simulate larger batch)
# (requires code modification in trainer)
```

### Issue: Model Not Learning (Loss Stuck)

**Possible causes**:
1. Learning rate too high/low
2. Data not normalized
3. Gradient vanishing/exploding

**Solutions**:
```bash
# Try different learning rate
python scripts/train_cnn.py --lr 0.0001

# Ensure normalization
python scripts/train_cnn.py --normalize

# Add gradient clipping
python scripts/train_cnn.py --grad-clip 1.0
```

### Issue: Overfitting (High Train Acc, Low Val Acc)

**Solutions**:
```bash
# Increase dropout
python scripts/train_cnn.py --dropout 0.5

# Use label smoothing
python scripts/train_cnn.py --loss label_smoothing --label-smoothing 0.1

# Early stopping
python scripts/train_cnn.py --early-stopping --patience 10
```

### Issue: Training Too Slow

**Solutions**:
```bash
# Use mixed precision
python scripts/train_cnn.py --mixed-precision

# Increase num workers
python scripts/train_cnn.py --num-workers 8

# Use lighter model
python scripts/train_cnn.py --model attention-lite
```

---

## ðŸ“ Directory Structure After Phase 2

```
LSTM_PFD/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ cnn/                       # NEW: CNN architectures
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ conv_blocks.py         # Modular conv blocks
â”‚       â”œâ”€â”€ cnn_1d.py              # Baseline CNN
â”‚       â”œâ”€â”€ attention_cnn.py       # Attention-based CNN
â”‚       â””â”€â”€ multi_scale_cnn.py     # Multi-scale CNN
â”‚
â”œâ”€â”€ data/                          # UPDATED: Add CNN data pipeline
â”‚   â”œâ”€â”€ cnn_dataset.py             # NEW: RawSignalDataset
â”‚   â”œâ”€â”€ cnn_dataloader.py          # NEW: CNN DataLoaders
â”‚   â”œâ”€â”€ cnn_transforms.py          # NEW: Signal transforms
â”‚   â””â”€â”€ signal_augmentation.py     # NEW: Advanced augmentation
â”‚
â”œâ”€â”€ training/                      # UPDATED: Add CNN training
â”‚   â”œâ”€â”€ cnn_trainer.py             # NEW: CNN trainer
â”‚   â”œâ”€â”€ cnn_optimizer.py           # NEW: Optimizer configs
â”‚   â”œâ”€â”€ cnn_losses.py              # NEW: Loss functions
â”‚   â”œâ”€â”€ cnn_schedulers.py          # NEW: LR schedulers
â”‚   â””â”€â”€ cnn_callbacks.py           # NEW: Training callbacks
â”‚
â”œâ”€â”€ evaluation/                    # UPDATED: Add CNN evaluation
â”‚   â””â”€â”€ cnn_evaluator.py           # NEW: CNN evaluator
â”‚
â”œâ”€â”€ experiments/                   # UPDATED: Add CNN experiments
â”‚   â””â”€â”€ cnn_experiment.py          # NEW: Experiment orchestration
â”‚
â”œâ”€â”€ visualization/                 # UPDATED: Add CNN visualization
â”‚   â”œâ”€â”€ cnn_visualizer.py          # NEW: Filter/activation viz
â”‚   â””â”€â”€ cnn_analysis.py            # NEW: Model analysis
â”‚
â”œâ”€â”€ scripts/                       # NEW: Command-line tools
â”‚   â”œâ”€â”€ train_cnn.py               # Training script
â”‚   â”œâ”€â”€ evaluate_cnn.py            # Evaluation script
â”‚   â””â”€â”€ inference_cnn.py           # Inference script
â”‚
â”œâ”€â”€ utils/                         # UPDATED: Add utilities
â”‚   â”œâ”€â”€ checkpoint_manager.py      # NEW: Checkpointing
â”‚   â””â”€â”€ early_stopping.py          # NEW: Early stopping
â”‚
â”œâ”€â”€ PHASE_2_USAGE_GUIDE.md         # This guide
â””â”€â”€ checkpoints/                   # Model checkpoints (created during training)
```

---

## ðŸŽ“ Understanding the CNN Pipeline

### Why CNNs for Bearing Fault Diagnosis?

**Advantages over Classical ML (Phase 1)**:
1. **End-to-end learning**: No manual feature engineering
2. **Automatic feature extraction**: Learns optimal features from raw signals
3. **Hierarchical features**: Low-level (frequency) â†’ High-level (fault patterns)
4. **Better generalization**: Less sensitive to noise and variations

**Trade-offs**:
- Requires more data (~1000+ samples per class)
- Longer training time (hours vs minutes)
- Less interpretable (black box)
- Requires GPU for efficient training

### CNN Architecture Design Choices

**Why 1D CNNs (not 2D)?**
- Signals are 1D time-series
- More efficient than spectrograms
- Direct time-domain processing

**Why 5 Convolutional Layers?**
- Balance between capacity and overfitting
- Sufficient receptive field for fault patterns
- Manageable training time

**Why Global Average Pooling?**
- Reduces parameters (vs fully connected)
- More robust to signal variations
- Acts as regularization

---

## ðŸ“š Next Steps

After Phase 2, you can:

1. **Phase 3**: Advanced CNNs (ResNet-18, EfficientNet)
2. **Phase 4**: Transformer models for sequential modeling
3. **Phase 5**: Time-frequency CNNs (spectrograms + 2D CNNs)
4. **Phase 6**: Physics-Informed Neural Networks (PINNs)
5. **Phase 7**: Explainable AI (SHAP, LIME)
6. **Phase 8**: Model ensemble (voting, stacking)
7. **Phase 9**: Deployment (quantization, ONNX, API)

---

## ðŸ’¡ Best Practices

1. **Always normalize signals** before training
2. **Use label smoothing** (0.1) for better calibration
3. **Enable mixed precision** for 2-3x speedup
4. **Monitor gradient flow** to detect vanishing gradients
5. **Use early stopping** (patience=10-15) to prevent overfitting
6. **Save multiple checkpoints** (not just best)
7. **Visualize predictions** on failure cases
8. **Compare with Phase 1** classical ML baseline

---

## ðŸ“ž Support

If you encounter issues:
1. Check that Phase 0 infrastructure is working
2. Verify GPU drivers and CUDA installation (`torch.cuda.is_available()`)
3. Review error messages and adjust hyperparameters
4. Check the visualization tools to diagnose model behavior
5. Compare results with expected performance ranges

---

**Happy Deep Learning! ðŸ§ ðŸ”§**
