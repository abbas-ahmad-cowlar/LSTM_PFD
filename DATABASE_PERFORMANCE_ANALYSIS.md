# Database Performance Optimization - Professional Analysis

## Executive Summary

**Status**: âš ï¸ NEEDS REFINEMENT
**Issues Found**: Duplicate indexes, over-indexing, missing monitoring
**Risk Level**: Medium (duplicate indexes waste resources but don't break functionality)

## Issues Identified

### ðŸ”´ CRITICAL: Duplicate Index Creation

**Problem**: Many models already have column-level indexes (`index=True`), and additional indexes were created in `__table_args__`, causing duplicates.

**Impact**:
- Wasted disk space (each index consumes storage)
- Slower write operations (INSERT/UPDATE/DELETE must update multiple identical indexes)
- Maintenance overhead
- No performance benefit

**Examples of Duplicates**:

| Model | Column | Existing Index | Duplicate Added |
|-------|--------|----------------|-----------------|
| `api_key.py` | `user_id` | ForeignKey auto-index | `ix_api_keys_user_id` |
| `api_key.py` | `is_active` | `index=True` | Part of composite |
| `email_log.py` | `user_id` | `index=True` | Part of composite |
| `email_log.py` | `event_type` | `index=True` | Part of composite |
| `email_log.py` | `status` | `index=True` | Part of composite |
| `webhook_configuration.py` | `user_id` | `index=True` | Part of composite |
| `webhook_configuration.py` | `provider_type` | `index=True` | Part of composite |
| `webhook_configuration.py` | `is_active` | `index=True` | Part of composite |
| `experiment.py` | `status` | `index=True` | Part of composite |
| `notification_preference.py` | `user_id` | `index=True` | Part of composite |
| `notification_preference.py` | `event_type` | `index=True` | Part of composite |

### ðŸŸ¡ MEDIUM: Over-Indexing on Log Tables

**Problem**: Log tables (`email_log`, `webhook_log`, `api_request_log`) have extensive indexing.

**Impact**:
- Log tables are write-heavy (high INSERT volume)
- Each index slows down writes
- Log queries are typically time-range scans, not precision lookups

**Recommendation**: Use fewer indexes, rely on partitioning or time-series optimizations instead.

### ðŸŸ¡ MEDIUM: Missing Index Strategy Documentation

**Problem**: No documentation explaining:
- Which queries each index optimizes
- Expected query patterns
- Index maintenance schedule

### ðŸŸ¢ LOW: Connection Pool Could Be Optimized

**Current**: `pool_size=50, max_overflow=50` (100 total)
**Recommended**: `pool_size=30, max_overflow=30` (60 total) with additional settings

**Additional Settings Needed**:
- `pool_timeout=30` - Wait time for connection from pool
- `max_identifier_length=128` - For PostgreSQL compatibility

## Corrected Implementation Strategy

### 1. Index Deduplication Rules

**Rule 1**: Do NOT create single-column indexes in `__table_args__` if column already has:
- `index=True`
- `unique=True` (automatically indexed)
- ForeignKey (automatically indexed in PostgreSQL/MySQL)

**Rule 2**: Composite indexes are ONLY beneficial if:
- The query uses both columns in WHERE clause
- The leftmost column has high cardinality
- The combination is queried more frequently than individual columns

**Rule 3**: Indexes on columns with low cardinality (few distinct values) provide minimal benefit:
- Boolean columns: Consider carefully
- Status enums with 3-5 values: Often not worth it
- Exception: When combined with high-cardinality column in composite

### 2. Proper Index Strategy by Table Type

#### A. Transaction Tables (Low Write Volume)
Examples: `experiments`, `datasets`, `users`, `hpo_campaigns`

**Strategy**: Aggressive indexing is acceptable
- Index foreign keys (if not auto-indexed)
- Index commonly filtered columns
- Use composite indexes for multi-column queries

#### B. Log Tables (High Write Volume)
Examples: `email_log`, `webhook_log`, `api_request_log`, `training_runs`

**Strategy**: Minimal indexing
- ONLY index time columns for range queries
- Avoid indexing high-cardinality text columns (endpoint URLs, etc.)
- Consider table partitioning instead of indexing

#### C. Junction/Relationship Tables
Examples: `experiment_tags`, `saved_searches`

**Strategy**: Let unique constraints handle indexing
- UniqueConstraint automatically creates composite index
- Don't duplicate it in `__table_args__`

### 3. Recommended Index Configuration

#### Models Requiring Changes:

**api_key.py**:
```python
# REMOVE duplicate indexes on user_id, is_active (already indexed)
__table_args__ = (
    Index('ix_api_keys_created_at', 'created_at'),
    # Composite useful if queries filter: WHERE user_id=X AND is_active=true
    # But user_id already indexed, so only add if query pattern proves it's needed
)
```

**email_log.py**:
```python
# REMOVE duplicates on user_id, event_type, status
__table_args__ = (
    Index('idx_email_logs_sent_at', 'sent_at'),
    Index('ix_email_logs_created_at', 'created_at'),
    # Composite indexes on log tables should be minimal
)
```

**experiment.py**:
```python
# Keep only non-duplicate indexes
__table_args__ = (
    Index('ix_experiments_created_by', 'created_by'),
    Index('ix_experiments_created_at', 'created_at'),
    Index('ix_experiments_dataset_id', 'dataset_id'),
    Index('ix_experiments_hpo_campaign_id', 'hpo_campaign_id'),
    # REMOVE user_status composite - status already indexed
)
```

**notification_preference.py**:
```python
# UniqueConstraint already creates composite index on (user_id, event_type)
# REMOVE duplicate composite index
__table_args__ = (
    UniqueConstraint('user_id', 'event_type', name='uq_user_event_type'),
    # No additional indexes needed
)
```

## Connection Pool - Optimized Configuration

```python
engine = create_engine(
    DATABASE_URL,
    pool_pre_ping=True,        # âœ“ Verify connection health
    pool_size=30,              # Reduced from 50 (sufficient for 26 callbacks)
    max_overflow=30,           # Reduced from 50
    pool_recycle=3600,         # âœ“ Recycle every hour
    pool_timeout=30,           # NEW: Wait 30s for connection
    echo=False,                # âœ“ Disable query logging in production
    max_identifier_length=128, # NEW: PostgreSQL compatibility
)
```

**Rationale**:
- 26 callbacks Ã— 60% concurrency = ~16 concurrent connections needed
- pool_size=30 provides 87% headroom
- max_overflow=30 handles traffic spikes
- Total 60 connections is sufficient (vs. current 100)

## Query Performance Monitoring

### Add SQLAlchemy Event Listeners

```python
# dash_app/database/connection.py

from sqlalchemy import event
from sqlalchemy.engine import Engine
import time
import logging

logger = logging.getLogger('sqlalchemy.performance')

@event.listens_for(Engine, "before_cursor_execute")
def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    conn.info.setdefault('query_start_time', []).append(time.time())

@event.listens_for(Engine, "after_cursor_execute")
def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total = time.time() - conn.info['query_start_time'].pop(-1)
    if total > 1.0:  # Log slow queries (>1 second)
        logger.warning(f"Slow query ({total:.2f}s): {statement[:200]}")
```

### Add Connection Pool Monitoring

```python
@event.listens_for(Pool, "connect")
def receive_connect(dbapi_conn, connection_record):
    logger.debug("Connection pool: +1 connection")

@event.listens_for(Pool, "checkin")
def receive_checkin(dbapi_conn, connection_record):
    logger.debug(f"Connection returned to pool. Size: {engine.pool.size()}")
```

## Migration Strategy

### Option 1: Alembic Migration (Recommended)

```bash
# Create migration
alembic revision -m "Add performance indexes and fix duplicates"
```

Migration file structure:
```python
def upgrade():
    # 1. DROP duplicate indexes
    op.drop_index('ix_api_keys_user_id', 'api_keys')
    op.drop_index('ix_email_logs_user_status', 'email_logs')
    # ... etc

    # 2. CREATE new optimized indexes
    op.create_index('ix_experiments_created_at', 'experiments', ['created_at'])
    # ... etc

def downgrade():
    # Reverse operations
```

### Option 2: Manual SQL (For Review)

```sql
-- Check existing indexes
SELECT schemaname, tablename, indexname, indexdef
FROM pg_indexes
WHERE schemaname = 'public'
ORDER BY tablename, indexname;

-- Identify duplicates
SELECT tablename, COUNT(*) as index_count
FROM pg_indexes
WHERE schemaname = 'public'
GROUP BY tablename
HAVING COUNT(*) > 5  -- Tables with many indexes
ORDER BY index_count DESC;
```

## Success Criteria (Revised)

âœ… **Connection Pool**:
- [x] Increased from 10/20 to 30/30 (adequate)
- [x] Added pool_recycle
- [ ] Add pool_timeout
- [ ] Add max_identifier_length
- [x] Pre-ping enabled

âœ… **Indexes**:
- [ ] NO duplicate indexes (needs fixing)
- [ ] Composite indexes only where query patterns justify
- [ ] Log tables have minimal indexes
- [ ] All foreign keys indexed (verify auto-indexing)
- [ ] Created_at indexed on transaction tables only

âœ… **Monitoring**:
- [ ] Slow query logging enabled
- [ ] Connection pool metrics tracked
- [ ] Index usage statistics collected

âœ… **Documentation**:
- [ ] Index strategy documented
- [ ] Query patterns documented
- [ ] Maintenance schedule defined

## Next Steps - Priority Order

1. **[HIGH] Remove Duplicate Indexes** - Immediate
2. **[HIGH] Add Pool Timeout Settings** - Immediate
3. **[MEDIUM] Implement Query Performance Monitoring** - This week
4. **[MEDIUM] Create Alembic Migration** - This week
5. **[LOW] Set up Index Usage Monitoring** - Next sprint
6. **[LOW] Consider Table Partitioning for Logs** - Future optimization

## References

- [PostgreSQL Index Best Practices](https://www.postgresql.org/docs/current/indexes.html)
- [SQLAlchemy Connection Pooling](https://docs.sqlalchemy.org/en/14/core/pooling.html)
- [Database Indexing Strategies](https://use-the-index-luke.com/)

---

**Document Version**: 1.0
**Last Updated**: 2025-11-22
**Author**: Database Performance Audit
