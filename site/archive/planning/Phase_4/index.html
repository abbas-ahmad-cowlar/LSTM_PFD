<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Advanced Bearing Fault Diagnosis System - Production-Ready Research Platform"><meta name=author content="Syed Abbas Ahmad"><link href=https://abbas-ahmad-cowlar.github.io/LSTM_PFD/archive/planning/Phase_4/ rel=canonical><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>Phase 4 - LSTM PFD Documentation</title><link rel=stylesheet href=../../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#phase-4-transformer-architecture-for-time-series class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="LSTM PFD Documentation" class="md-header__button md-logo" aria-label="LSTM PFD Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m16 11.78 4.24-7.33 1.73 1-5.23 9.05-6.51-3.75L5.46 19H22v2H2V3h2v14.54L9.5 8z"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> LSTM PFD Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Phase 4 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/abbas-ahmad-cowlar/LSTM_PFD title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> abbas-ahmad-cowlar/LSTM_PFD </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../getting-started/ class=md-tabs__link> Getting Started </a> </li> <li class=md-tabs__item> <a href=../../../user-guide/ class=md-tabs__link> User Guide </a> </li> <li class=md-tabs__item> <a href=../../../api/ class=md-tabs__link> API Reference </a> </li> <li class=md-tabs__item> <a href=../../../research/ class=md-tabs__link> Research </a> </li> <li class=md-tabs__item> <a href=../../../DEPLOYMENT_GUIDE/ class=md-tabs__link> Deployment </a> </li> <li class=md-tabs__item> <a href=../../../../CHANGELOG.md class=md-tabs__link> Changelog </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="LSTM PFD Documentation" class="md-nav__button md-logo" aria-label="LSTM PFD Documentation" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m16 11.78 4.24-7.33 1.73 1-5.23 9.05-6.51-3.75L5.46 19H22v2H2V3h2v14.54L9.5 8z"/></svg> </a> LSTM PFD Documentation </label> <div class=md-nav__source> <a href=https://github.com/abbas-ahmad-cowlar/LSTM_PFD title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> abbas-ahmad-cowlar/LSTM_PFD </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Getting Started </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Getting Started </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../getting-started/ class=md-nav__link> <span class=md-ellipsis> Getting Started </span> </a> </li> <li class=md-nav__item> <a href=../../../getting-started/installation/ class=md-nav__link> <span class=md-ellipsis> Installation </span> </a> </li> <li class=md-nav__item> <a href=../../../getting-started/quickstart/ class=md-nav__link> <span class=md-ellipsis> Quick Start </span> </a> </li> <li class=md-nav__item> <a href=../../../getting-started/configuration/ class=md-nav__link> <span class=md-ellipsis> Configuration </span> </a> </li> <li class=md-nav__item> <a href=../../../getting-started/first-experiment/ class=md-nav__link> <span class=md-ellipsis> First Experiment </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> User Guide </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> User Guide </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../user-guide/ class=md-nav__link> <span class=md-ellipsis> User Guide </span> </a> </li> <li class=md-nav__item> <a href=../../../USAGE_PHASE_11/ class=md-nav__link> <span class=md-ellipsis> Dashboard Usage </span> </a> </li> <li class=md-nav__item> <a href=../../../USER_GUIDE/ class=md-nav__link> <span class=md-ellipsis> User Guide </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> API Reference </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> API Reference </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../api/ class=md-nav__link> <span class=md-ellipsis> API Reference </span> </a> </li> <li class=md-nav__item> <a href=../../../API_REFERENCE/ class=md-nav__link> <span class=md-ellipsis> REST API </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> <span class=md-ellipsis> Research </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Research </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../research/ class=md-nav__link> <span class=md-ellipsis> Research </span> </a> </li> <li class=md-nav__item> <a href=../../../research/pinn-theory/ class=md-nav__link> <span class=md-ellipsis> PINN Theory </span> </a> </li> <li class=md-nav__item> <a href=../../../research/xai-methods/ class=md-nav__link> <span class=md-ellipsis> XAI Methods </span> </a> </li> <li class=md-nav__item> <a href=../../../research/ensemble-strategies/ class=md-nav__link> <span class=md-ellipsis> Ensemble Strategies </span> </a> </li> <li class=md-nav__item> <a href=../../../research/ablation-studies/ class=md-nav__link> <span class=md-ellipsis> Ablation Studies </span> </a> </li> <li class=md-nav__item> <a href=../../../research/reproducibility/ class=md-nav__link> <span class=md-ellipsis> Reproducibility </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex=0> <span class=md-ellipsis> Deployment </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Deployment </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../DEPLOYMENT_GUIDE/ class=md-nav__link> <span class=md-ellipsis> Deployment Guide </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../CHANGELOG.md class=md-nav__link> <span class=md-ellipsis> Changelog </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#phase-4-transformer-architecture-for-time-series class=md-nav__link> <span class=md-ellipsis> PHASE 4: Transformer Architecture for Time-Series </span> </a> <nav class=md-nav aria-label="PHASE 4: Transformer Architecture for Time-Series"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#phase-objective class=md-nav__link> <span class=md-ellipsis> Phase Objective </span> </a> </li> <li class=md-nav__item> <a href=#complete-file-list-20-files class=md-nav__link> <span class=md-ellipsis> Complete File List (20 files) </span> </a> </li> <li class=md-nav__item> <a href=#architecture-decisions class=md-nav__link> <span class=md-ellipsis> Architecture Decisions </span> </a> </li> <li class=md-nav__item> <a href=#data-flow class=md-nav__link> <span class=md-ellipsis> Data Flow </span> </a> </li> <li class=md-nav__item> <a href=#integration-points class=md-nav__link> <span class=md-ellipsis> Integration Points </span> </a> </li> <li class=md-nav__item> <a href=#testing-strategy class=md-nav__link> <span class=md-ellipsis> Testing Strategy </span> </a> </li> <li class=md-nav__item> <a href=#acceptance-criteria class=md-nav__link> <span class=md-ellipsis> Acceptance Criteria </span> </a> </li> <li class=md-nav__item> <a href=#estimated-effort class=md-nav__link> <span class=md-ellipsis> Estimated Effort </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1>Phase 4</h1> <h2 id=phase-4-transformer-architecture-for-time-series><strong>PHASE 4: Transformer Architecture for Time-Series</strong><a class=headerlink href=#phase-4-transformer-architecture-for-time-series title="Permanent link">&para;</a></h2> <h3 id=phase-objective>Phase Objective<a class=headerlink href=#phase-objective title="Permanent link">&para;</a></h3> <p>Implement Transformer encoder architecture adapted for vibration signal classification, leveraging self-attention mechanisms to capture long-range temporal dependencies that CNNs may miss. Explore patch-based embeddings and compare attention patterns to ResNet activations. Target: Match or exceed ResNet-50 accuracy (96-97%) while providing interpretable attention maps showing which time regions drive fault classification.</p> <h3 id=complete-file-list-20-files>Complete File List (20 files)<a class=headerlink href=#complete-file-list-20-files title="Permanent link">&para;</a></h3> <h4 id=1-transformer-core-5-files><strong>1. Transformer Core (5 files)</strong><a class=headerlink href=#1-transformer-core-5-files title="Permanent link">&para;</a></h4> <p><strong><code>models/transformer/signal_transformer.py</code></strong> - <strong>Purpose</strong>: Main Transformer encoder for fault diagnosis - <strong>Key Classes</strong>: - <code>SignalTransformer(BaseModel)</code>: Complete Transformer architecture - <code>PatchEmbedding(nn.Module)</code>: Convert signal to sequence of patches - <code>PositionalEncoding(nn.Module)</code>: Learnable or sinusoidal position embeddings - <strong>Architecture</strong>: <div class="language-text highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>Input: [B, 1, 102400]
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>1. Patch Embedding:
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>   ├─ Divide signal into patches: 102400 / 512 = 200 patches
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>   ├─ Each patch: [512] samples
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>   ├─ Linear projection: 512 → d_model (e.g., 256)
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>   └─ Output: [B, 200, 256]  # (batch, seq_len, embed_dim)
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>2. Positional Encoding:
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>   ├─ Add learnable position embeddings
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a>   └─ Output: [B, 200, 256]
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a>
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a>3. Transformer Encoder (6 layers):
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a>   For each layer:
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a>     ├─ Multi-Head Self-Attention (8 heads)
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a>     │   ├─ Q, K, V = Linear(x)
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a>     │   ├─ Attention(Q, K, V) = softmax(QK^T / √d_k) V
</span><span id=__span-0-18><a id=__codelineno-0-18 name=__codelineno-0-18 href=#__codelineno-0-18></a>     │   └─ Concatenate heads, project
</span><span id=__span-0-19><a id=__codelineno-0-19 name=__codelineno-0-19 href=#__codelineno-0-19></a>     ├─ Add &amp; Norm (residual + LayerNorm)
</span><span id=__span-0-20><a id=__codelineno-0-20 name=__codelineno-0-20 href=#__codelineno-0-20></a>     ├─ Feed-Forward Network (d_model → 4*d_model → d_model)
</span><span id=__span-0-21><a id=__codelineno-0-21 name=__codelineno-0-21 href=#__codelineno-0-21></a>     └─ Add &amp; Norm
</span><span id=__span-0-22><a id=__codelineno-0-22 name=__codelineno-0-22 href=#__codelineno-0-22></a>   Output: [B, 200, 256]
</span><span id=__span-0-23><a id=__codelineno-0-23 name=__codelineno-0-23 href=#__codelineno-0-23></a>
</span><span id=__span-0-24><a id=__codelineno-0-24 name=__codelineno-0-24 href=#__codelineno-0-24></a>4. Classification Head:
</span><span id=__span-0-25><a id=__codelineno-0-25 name=__codelineno-0-25 href=#__codelineno-0-25></a>   ├─ Global average pooling over sequence: [B, 200, 256] → [B, 256]
</span><span id=__span-0-26><a id=__codelineno-0-26 name=__codelineno-0-26 href=#__codelineno-0-26></a>   └─ FC: 256 → 11
</span></code></pre></div> - <strong>Key Functions</strong>: - <code>forward(x)</code>: Full forward pass - <code>get_attention_weights(x, layer_idx)</code>: Extract attention maps for interpretability - <strong>Hyperparameters</strong>: - d_model: 256 (embedding dimension) - n_heads: 8 (multi-head attention) - n_layers: 6 (transformer blocks) - d_ff: 1024 (feedforward hidden dim, 4 × d_model) - dropout: 0.1 - <strong>Parameters</strong>: ~5M (comparable to ResNet-34) - <strong>Dependencies</strong>: <code>torch.nn</code>, <code>models/base_model.py</code></p> <p><strong><code>models/transformer/patch_embedding.py</code></strong> - <strong>Purpose</strong>: Convert 1D signal to sequence of patch embeddings - <strong>Key Classes</strong>: - <code>PatchEmbedding1D(nn.Module)</code>: Patch extraction + projection - <strong>Patching Strategies</strong>: <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=c1># Strategy 1: Non-overlapping patches</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=n>patches</span> <span class=o>=</span> <span class=n>signal</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>patch_size</span><span class=p>)</span>  <span class=c1># [B, n_patches, patch_size]</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=n>embeddings</span> <span class=o>=</span> <span class=n>linear</span><span class=p>(</span><span class=n>patches</span><span class=p>)</span>  <span class=c1># [B, n_patches, d_model]</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=c1># Strategy 2: Overlapping patches (stride &lt; patch_size)</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a><span class=n>patches</span> <span class=o>=</span> <span class=n>unfold</span><span class=p>(</span><span class=n>signal</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=n>patch_size</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=n>stride</span><span class=p>)</span>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=n>embeddings</span> <span class=o>=</span> <span class=n>linear</span><span class=p>(</span><span class=n>patches</span><span class=p>)</span>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a><span class=c1># Strategy 3: Convolutional embedding (1D conv with large kernel)</span>
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a><span class=n>embeddings</span> <span class=o>=</span> <span class=n>Conv1D</span><span class=p>(</span><span class=n>in_channels</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>out_channels</span><span class=o>=</span><span class=n>d_model</span><span class=p>,</span> 
</span><span id=__span-1-11><a id=__codelineno-1-11 name=__codelineno-1-11 href=#__codelineno-1-11></a>                    <span class=n>kernel_size</span><span class=o>=</span><span class=n>patch_size</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=n>patch_size</span><span class=p>)(</span><span class=n>signal</span><span class=p>)</span>
</span></code></pre></div> - <strong>Key Functions</strong>: - <code>forward(x)</code>: [B, 1, T] → [B, n_patches, d_model] - <strong>Recommended</strong>: patch_size=512, stride=512 (non-overlapping) - <strong>Dependencies</strong>: <code>torch.nn</code></p> <p><strong><code>models/transformer/positional_encoding.py</code></strong> - <strong>Purpose</strong>: Add position information to patch embeddings - <strong>Key Classes</strong>: - <code>LearnablePositionalEncoding(nn.Module)</code>: Trainable embeddings - <code>SinusoidalPositionalEncoding(nn.Module)</code>: Fixed sin/cos encoding (Vaswani et al.) - <code>RelativePositionalEncoding(nn.Module)</code>: Relative positions (more robust) - <strong>Sinusoidal Formula</strong> (original Transformer): <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=n>PE</span><span class=p>(</span><span class=n>pos</span><span class=p>,</span> <span class=mi>2</span><span class=n>i</span><span class=p>)</span> <span class=o>=</span> <span class=n>sin</span><span class=p>(</span><span class=n>pos</span> <span class=o>/</span> <span class=mi>10000</span><span class=o>^</span><span class=p>(</span><span class=mi>2</span><span class=n>i</span><span class=o>/</span><span class=n>d_model</span><span class=p>))</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=n>PE</span><span class=p>(</span><span class=n>pos</span><span class=p>,</span> <span class=mi>2</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>)</span> <span class=o>=</span> <span class=n>cos</span><span class=p>(</span><span class=n>pos</span> <span class=o>/</span> <span class=mi>10000</span><span class=o>^</span><span class=p>(</span><span class=mi>2</span><span class=n>i</span><span class=o>/</span><span class=n>d_model</span><span class=p>))</span>
</span></code></pre></div> - <strong>Key Functions</strong>: - <code>forward(x)</code>: Add positional encoding to input - <strong>Recommendation</strong>: Use learnable for flexibility - <strong>Dependencies</strong>: <code>torch.nn</code></p> <p><strong><code>models/transformer/multi_head_attention.py</code></strong> - <strong>Purpose</strong>: Multi-head self-attention mechanism - <strong>Key Classes</strong>: - <code>MultiHeadAttention(nn.Module)</code>: Parallel attention heads - <strong>Implementation</strong>: <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=k>class</span><span class=w> </span><span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>):</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>        <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>n_heads</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>        <span class=bp>self</span><span class=o>.</span><span class=n>n_heads</span> <span class=o>=</span> <span class=n>n_heads</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>        <span class=bp>self</span><span class=o>.</span><span class=n>q_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a>        <span class=bp>self</span><span class=o>.</span><span class=n>k_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a>        <span class=bp>self</span><span class=o>.</span><span class=n>v_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a>        <span class=bp>self</span><span class=o>.</span><span class=n>out_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a>        <span class=n>B</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a>
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13 href=#__codelineno-3-13></a>        <span class=c1># Linear projections and split into heads</span>
</span><span id=__span-3-14><a id=__codelineno-3-14 name=__codelineno-3-14 href=#__codelineno-3-14></a>        <span class=n>Q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span><span id=__span-3-15><a id=__codelineno-3-15 name=__codelineno-3-15 href=#__codelineno-3-15></a>        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span><span id=__span-3-16><a id=__codelineno-3-16 name=__codelineno-3-16 href=#__codelineno-3-16></a>        <span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>v_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span><span id=__span-3-17><a id=__codelineno-3-17 name=__codelineno-3-17 href=#__codelineno-3-17></a>        <span class=c1># Q, K, V: [B, n_heads, seq_len, d_k]</span>
</span><span id=__span-3-18><a id=__codelineno-3-18 name=__codelineno-3-18 href=#__codelineno-3-18></a>
</span><span id=__span-3-19><a id=__codelineno-3-19 name=__codelineno-3-19 href=#__codelineno-3-19></a>        <span class=c1># Scaled dot-product attention</span>
</span><span id=__span-3-20><a id=__codelineno-3-20 name=__codelineno-3-20 href=#__codelineno-3-20></a>        <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span>
</span><span id=__span-3-21><a id=__codelineno-3-21 name=__codelineno-3-21 href=#__codelineno-3-21></a>        <span class=c1># scores: [B, n_heads, seq_len, seq_len]</span>
</span><span id=__span-3-22><a id=__codelineno-3-22 name=__codelineno-3-22 href=#__codelineno-3-22></a>
</span><span id=__span-3-23><a id=__codelineno-3-23 name=__codelineno-3-23 href=#__codelineno-3-23></a>        <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-3-24><a id=__codelineno-3-24 name=__codelineno-3-24 href=#__codelineno-3-24></a>            <span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1e9</span><span class=p>)</span>
</span><span id=__span-3-25><a id=__codelineno-3-25 name=__codelineno-3-25 href=#__codelineno-3-25></a>
</span><span id=__span-3-26><a id=__codelineno-3-26 name=__codelineno-3-26 href=#__codelineno-3-26></a>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># Attention probabilities</span>
</span><span id=__span-3-27><a id=__codelineno-3-27 name=__codelineno-3-27 href=#__codelineno-3-27></a>        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>  <span class=c1># [B, n_heads, seq_len, d_k]</span>
</span><span id=__span-3-28><a id=__codelineno-3-28 name=__codelineno-3-28 href=#__codelineno-3-28></a>
</span><span id=__span-3-29><a id=__codelineno-3-29 name=__codelineno-3-29 href=#__codelineno-3-29></a>        <span class=c1># Concatenate heads</span>
</span><span id=__span-3-30><a id=__codelineno-3-30 name=__codelineno-3-30 href=#__codelineno-3-30></a>        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span><span id=__span-3-31><a id=__codelineno-3-31 name=__codelineno-3-31 href=#__codelineno-3-31></a>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_linear</span><span class=p>(</span><span class=n>attn_output</span><span class=p>)</span>
</span><span id=__span-3-32><a id=__codelineno-3-32 name=__codelineno-3-32 href=#__codelineno-3-32></a>
</span><span id=__span-3-33><a id=__codelineno-3-33 name=__codelineno-3-33 href=#__codelineno-3-33></a>        <span class=k>return</span> <span class=n>output</span><span class=p>,</span> <span class=n>attn_weights</span>  <span class=c1># Return weights for visualization</span>
</span></code></pre></div> - <strong>Key Functions</strong>: - <code>forward(x, mask)</code>: Compute attention output and weights - <strong>Dependencies</strong>: <code>torch.nn</code>, <code>torch.nn.functional</code></p> <p><strong><code>models/transformer/transformer_encoder_layer.py</code></strong> - <strong>Purpose</strong>: Single Transformer encoder block - <strong>Key Classes</strong>: - <code>TransformerEncoderLayer(nn.Module)</code>: Attention + FFN + residual - <strong>Implementation</strong>: <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=k>class</span><span class=w> </span><span class=nc>TransformerEncoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>        <span class=bp>self</span><span class=o>.</span><span class=n>attention</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>)</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a>        <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a>        <span class=bp>self</span><span class=o>.</span><span class=n>ffn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>),</span>
</span><span id=__span-4-7><a id=__codelineno-4-7 name=__codelineno-4-7 href=#__codelineno-4-7></a>            <span class=n>nn</span><span class=o>.</span><span class=n>GELU</span><span class=p>(),</span>
</span><span id=__span-4-8><a id=__codelineno-4-8 name=__codelineno-4-8 href=#__codelineno-4-8></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>),</span>
</span><span id=__span-4-9><a id=__codelineno-4-9 name=__codelineno-4-9 href=#__codelineno-4-9></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_ff</span><span class=p>,</span> <span class=n>d_model</span><span class=p>),</span>
</span><span id=__span-4-10><a id=__codelineno-4-10 name=__codelineno-4-10 href=#__codelineno-4-10></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span><span id=__span-4-11><a id=__codelineno-4-11 name=__codelineno-4-11 href=#__codelineno-4-11></a>        <span class=p>)</span>
</span><span id=__span-4-12><a id=__codelineno-4-12 name=__codelineno-4-12 href=#__codelineno-4-12></a>        <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span><span id=__span-4-13><a id=__codelineno-4-13 name=__codelineno-4-13 href=#__codelineno-4-13></a>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span><span id=__span-4-14><a id=__codelineno-4-14 name=__codelineno-4-14 href=#__codelineno-4-14></a>
</span><span id=__span-4-15><a id=__codelineno-4-15 name=__codelineno-4-15 href=#__codelineno-4-15></a>    <span class=k>def</span><span class=w> </span><span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span><span id=__span-4-16><a id=__codelineno-4-16 name=__codelineno-4-16 href=#__codelineno-4-16></a>        <span class=c1># Multi-head attention with residual</span>
</span><span id=__span-4-17><a id=__codelineno-4-17 name=__codelineno-4-17 href=#__codelineno-4-17></a>        <span class=n>attn_output</span><span class=p>,</span> <span class=n>attn_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-4-18><a id=__codelineno-4-18 name=__codelineno-4-18 href=#__codelineno-4-18></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attn_output</span><span class=p>))</span>
</span><span id=__span-4-19><a id=__codelineno-4-19 name=__codelineno-4-19 href=#__codelineno-4-19></a>
</span><span id=__span-4-20><a id=__codelineno-4-20 name=__codelineno-4-20 href=#__codelineno-4-20></a>        <span class=c1># Feed-forward network with residual</span>
</span><span id=__span-4-21><a id=__codelineno-4-21 name=__codelineno-4-21 href=#__codelineno-4-21></a>        <span class=n>ffn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ffn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-4-22><a id=__codelineno-4-22 name=__codelineno-4-22 href=#__codelineno-4-22></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=n>ffn_output</span><span class=p>)</span>
</span><span id=__span-4-23><a id=__codelineno-4-23 name=__codelineno-4-23 href=#__codelineno-4-23></a>
</span><span id=__span-4-24><a id=__codelineno-4-24 name=__codelineno-4-24 href=#__codelineno-4-24></a>        <span class=k>return</span> <span class=n>x</span><span class=p>,</span> <span class=n>attn_weights</span>
</span></code></pre></div> - <strong>Key Functions</strong>: - <code>forward(x)</code>: Apply full encoder layer - <strong>Dependencies</strong>: <code>multi_head_attention.py</code></p> <h4 id=2-transformer-variants-4-files><strong>2. Transformer Variants (4 files)</strong><a class=headerlink href=#2-transformer-variants-4-files title="Permanent link">&para;</a></h4> <p><strong><code>models/transformer/vision_transformer_1d.py</code></strong> - <strong>Purpose</strong>: Vision Transformer (ViT) adapted for 1D signals - <strong>Key Classes</strong>: - <code>ViT1D(SignalTransformer)</code>: ViT with cls token - <strong>Architecture Modification</strong>: <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=c1># Standard Transformer:</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a><span class=n>Input</span> <span class=n>patches</span> <span class=err>→</span> <span class=n>Transformer</span> <span class=err>→</span> <span class=n>Global</span> <span class=n>avg</span> <span class=n>pool</span> <span class=err>→</span> <span class=n>FC</span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a><span class=c1># ViT:</span>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a><span class=p>[</span><span class=n>CLS</span><span class=p>]</span> <span class=n>token</span> <span class=o>+</span> <span class=n>Input</span> <span class=n>patches</span> <span class=err>→</span> <span class=n>Transformer</span> <span class=err>→</span> <span class=p>[</span><span class=n>CLS</span><span class=p>]</span> <span class=n>output</span> <span class=err>→</span> <span class=n>FC</span>
</span></code></pre></div> - Prepend learnable [CLS] token to sequence - Use [CLS] token output for classification (instead of avg pooling) - <strong>Benefit</strong>: [CLS] token learns task-relevant global representation - <strong>Dependencies</strong>: <code>signal_transformer.py</code></p> <p><strong><code>models/transformer/performer.py</code></strong> - <strong>Purpose</strong>: Performer (efficient attention) for long signals - <strong>Key Classes</strong>: - <code>Performer1D(SignalTransformer)</code>: Linear-complexity attention - <strong>Problem with Standard Attention</strong>: - Attention complexity: O(n²) where n = sequence length - For 200 patches: 200×200 = 40,000 attention computations - Becomes prohibitive for longer signals - <strong>Performer Solution</strong>: - Approximate attention with random features - Complexity: O(n) (linear in sequence length) - Slight accuracy trade-off (~1%) for 10× speedup - <strong>Key Functions</strong>: - <code>forward(x)</code>: Same interface as standard Transformer - <strong>Dependencies</strong>: <code>torch.nn</code>, <code>performer-pytorch</code> library</p> <p><strong><code>models/transformer/temporal_fusion_transformer.py</code></strong> - <strong>Purpose</strong>: Temporal Fusion Transformer (TFT) with gating mechanisms - <strong>Key Classes</strong>: - <code>TFT1D(SignalTransformer)</code>: TFT adapted for fault diagnosis - <code>GatedResidualNetwork(nn.Module)</code>: Gating for feature selection - <strong>Additions to Standard Transformer</strong>: - Variable selection: Learn which patches are important - Gated residual connections: Adaptive skip connections - Static covariate encoder: Incorporate operating condition metadata - <strong>Use Case</strong>: When metadata available (load, speed, temperature) - <strong>Dependencies</strong>: <code>signal_transformer.py</code></p> <p><strong><code>models/transformer/informer.py</code></strong> - <strong>Purpose</strong>: Informer (efficient long-sequence Transformer) - <strong>Key Classes</strong>: - <code>Informer1D(SignalTransformer)</code>: ProbSparse attention - <strong>ProbSparse Attention</strong>: - Select top-k most important queries (not all queries attend to all keys) - Complexity: O(n log n) - Better for very long signals (&gt; 50,000 samples) - <strong>Dependencies</strong>: <code>signal_transformer.py</code></p> <h4 id=3-training-enhancements-for-transformer-3-files><strong>3. Training Enhancements for Transformer (3 files)</strong><a class=headerlink href=#3-training-enhancements-for-transformer-3-files title="Permanent link">&para;</a></h4> <p><strong><code>training/transformer_trainer.py</code></strong> - <strong>Purpose</strong>: Transformer-specific training loop - <strong>Key Classes</strong>: - <code>TransformerTrainer(Trainer)</code>: Extends base trainer - <strong>Transformer-Specific Considerations</strong>: <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=c1># Learning rate warmup (critical for Transformers)</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a><span class=k>def</span><span class=w> </span><span class=nf>lr_schedule</span><span class=p>(</span><span class=n>epoch</span><span class=p>):</span>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>    <span class=k>if</span> <span class=n>epoch</span> <span class=o>&lt;</span> <span class=n>warmup_epochs</span><span class=p>:</span>
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>        <span class=k>return</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=n>warmup_epochs</span>  <span class=c1># Linear warmup</span>
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a>        <span class=k>return</span> <span class=n>cosine_annealing</span><span class=p>(</span><span class=n>epoch</span> <span class=o>-</span> <span class=n>warmup_epochs</span><span class=p>)</span>
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a>
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a><span class=c1># Gradient clipping (prevent exploding gradients)</span>
</span><span id=__span-6-9><a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>clip_grad_norm_</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>max_norm</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
</span><span id=__span-6-10><a id=__codelineno-6-10 name=__codelineno-6-10 href=#__codelineno-6-10></a>
</span><span id=__span-6-11><a id=__codelineno-6-11 name=__codelineno-6-11 href=#__codelineno-6-11></a><span class=c1># Label smoothing (Transformers benefit more than CNNs)</span>
</span><span id=__span-6-12><a id=__codelineno-6-12 name=__codelineno-6-12 href=#__codelineno-6-12></a><span class=n>loss_fn</span> <span class=o>=</span> <span class=n>LabelSmoothingCrossEntropy</span><span class=p>(</span><span class=n>smoothing</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</span></code></pre></div> - <strong>Key Functions</strong>: - <code>train_epoch(dataloader)</code>: Training with warmup schedule - <code>_update_lr(epoch)</code>: Custom LR schedule - <strong>Dependencies</strong>: <code>training/trainer.py</code></p> <p><strong><code>training/transformer_augmentation.py</code></strong> - <strong>Purpose</strong>: Augmentation specific to patch-based models - <strong>Key Functions</strong>: - <code>patch_dropout(patches, drop_prob=0.1)</code>: Randomly drop patches (regularization) - <code>patch_mixup(patches1, patches2, alpha)</code>: Mix patches from two signals - <code>temporal_shift_patches(patches, shift)</code>: Shift patches temporally - <strong>Patch Dropout</strong>: Similar to DropBlock in CNNs <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=c1># Randomly drop 10% of patches during training</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a><span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>n_patches</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> <span class=o>&gt;</span> <span class=n>drop_prob</span>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a><span class=n>patches</span> <span class=o>=</span> <span class=n>patches</span> <span class=o>*</span> <span class=n>mask</span>
</span></code></pre></div> - <strong>Dependencies</strong>: <code>numpy</code>, <code>torch</code></p> <p><strong><code>training/transformer_schedulers.py</code></strong> - <strong>Purpose</strong>: Learning rate schedules tailored for Transformers - <strong>Key Functions</strong>: - <code>create_warmup_cosine_schedule(optimizer, warmup_epochs, total_epochs)</code>: <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=k>def</span><span class=w> </span><span class=nf>lr_lambda</span><span class=p>(</span><span class=n>epoch</span><span class=p>):</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a>    <span class=k>if</span> <span class=n>epoch</span> <span class=o>&lt;</span> <span class=n>warmup_epochs</span><span class=p>:</span>
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a>        <span class=k>return</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=n>warmup_epochs</span>
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a>        <span class=n>progress</span> <span class=o>=</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>-</span> <span class=n>warmup_epochs</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>total_epochs</span> <span class=o>-</span> <span class=n>warmup_epochs</span><span class=p>)</span>
</span><span id=__span-8-6><a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a>        <span class=k>return</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>math</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>pi</span> <span class=o>*</span> <span class=n>progress</span><span class=p>))</span>
</span><span id=__span-8-7><a id=__codelineno-8-7 name=__codelineno-8-7 href=#__codelineno-8-7></a><span class=k>return</span> <span class=n>LambdaLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>lr_lambda</span><span class=p>)</span>
</span></code></pre></div> - <code>create_noam_schedule(optimizer, d_model, warmup_steps)</code>: Original Transformer schedule - <strong>Dependencies</strong>: <code>torch.optim.lr_scheduler</code></p> <h4 id=4-interpretability-and-visualization-4-files><strong>4. Interpretability and Visualization (4 files)</strong><a class=headerlink href=#4-interpretability-and-visualization-4-files title="Permanent link">&para;</a></h4> <p><strong><code>evaluation/attention_visualization.py</code></strong> - <strong>Purpose</strong>: Visualize attention maps to understand model decisions - <strong>Key Functions</strong>: - <code>plot_attention_heatmap(attention_weights, signal, patch_size)</code>: <div class="language-python highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class=c1># attention_weights: [n_heads, n_patches, n_patches]</span>
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a><span class=c1># Average over heads</span>
</span><span id=__span-9-3><a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a><span class=n>avg_attention</span> <span class=o>=</span> <span class=n>attention_weights</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>  <span class=c1># [n_patches, n_patches]</span>
</span><span id=__span-9-4><a id=__codelineno-9-4 name=__codelineno-9-4 href=#__codelineno-9-4></a>
</span><span id=__span-9-5><a id=__codelineno-9-5 name=__codelineno-9-5 href=#__codelineno-9-5></a><span class=c1># For each patch, show which other patches it attends to</span>
</span><span id=__span-9-6><a id=__codelineno-9-6 name=__codelineno-9-6 href=#__codelineno-9-6></a><span class=k>for</span> <span class=n>patch_idx</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_patches</span><span class=p>):</span>
</span><span id=__span-9-7><a id=__codelineno-9-7 name=__codelineno-9-7 href=#__codelineno-9-7></a>    <span class=n>attn_scores</span> <span class=o>=</span> <span class=n>avg_attention</span><span class=p>[</span><span class=n>patch_idx</span><span class=p>,</span> <span class=p>:]</span>  <span class=c1># Attention from patch_idx</span>
</span><span id=__span-9-8><a id=__codelineno-9-8 name=__codelineno-9-8 href=#__codelineno-9-8></a>    <span class=c1># Visualize as heatmap overlaid on signal</span>
</span><span id=__span-9-9><a id=__codelineno-9-9 name=__codelineno-9-9 href=#__codelineno-9-9></a>    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>signal</span><span class=p>)</span>
</span><span id=__span-9-10><a id=__codelineno-9-10 name=__codelineno-9-10 href=#__codelineno-9-10></a>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>score</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>attn_scores</span><span class=p>):</span>
</span><span id=__span-9-11><a id=__codelineno-9-11 name=__codelineno-9-11 href=#__codelineno-9-11></a>        <span class=n>start_sample</span> <span class=o>=</span> <span class=n>i</span> <span class=o>*</span> <span class=n>patch_size</span>
</span><span id=__span-9-12><a id=__codelineno-9-12 name=__codelineno-9-12 href=#__codelineno-9-12></a>        <span class=n>end_sample</span> <span class=o>=</span> <span class=p>(</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=n>patch_size</span>
</span><span id=__span-9-13><a id=__codelineno-9-13 name=__codelineno-9-13 href=#__codelineno-9-13></a>        <span class=n>plt</span><span class=o>.</span><span class=n>axvspan</span><span class=p>(</span><span class=n>start_sample</span><span class=p>,</span> <span class=n>end_sample</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=n>score</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>)</span>
</span></code></pre></div> - <code>plot_attention_rollout(model, signal)</code>: Aggregate attention across layers - <code>find_most_attended_patches(attention_weights)</code>: Identify key time regions - <strong>Use Case</strong>: - Explain why model classified signal as "misalignment" - Show attention focuses on 2X harmonic regions (expected for misalignment) - <strong>Dependencies</strong>: <code>matplotlib</code>, <code>torch</code></p> <p><strong><code>evaluation/transformer_interpretability.py</code></strong> - <strong>Purpose</strong>: Attribution methods for Transformers - <strong>Key Functions</strong>: - <code>attention_rollout(model, signal, target_class)</code>: Aggregate attention from all layers <div class="language-python highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=c1># Recursively multiply attention matrices</span>
</span><span id=__span-10-2><a id=__codelineno-10-2 name=__codelineno-10-2 href=#__codelineno-10-2></a><span class=n>rollout</span> <span class=o>=</span> <span class=n>attention_layer1</span> <span class=o>@</span> <span class=n>attention_layer2</span> <span class=o>@</span> <span class=o>...</span> <span class=o>@</span> <span class=n>attention_layer6</span>
</span><span id=__span-10-3><a id=__codelineno-10-3 name=__codelineno-10-3 href=#__codelineno-10-3></a><span class=c1># rollout[i, j] = importance of patch j for patch i</span>
</span></code></pre></div> - <code>attention_flow(model, signal)</code>: Visualize information flow through layers - <code>patch_attribution(model, signal, target_class)</code>: Which patches contribute to prediction - <strong>Dependencies</strong>: <code>torch</code>, <code>attention_visualization.py</code></p> <p><strong><code>evaluation/compare_attention_vs_gradcam.py</code></strong> - <strong>Purpose</strong>: Compare Transformer attention to CNN Grad-CAM - <strong>Key Functions</strong>: - <code>compare_interpretability(transformer_model, cnn_model, signal, label)</code>: <div class="language-python highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a><span class=c1># Transformer: Extract attention weights</span>
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a><span class=n>attn_weights</span> <span class=o>=</span> <span class=n>transformer_model</span><span class=o>.</span><span class=n>get_attention_weights</span><span class=p>(</span><span class=n>signal</span><span class=p>,</span> <span class=n>layer_idx</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a><span class=n>attn_importance</span> <span class=o>=</span> <span class=n>attn_weights</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>  <span class=c1># Average over heads</span>
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a>
</span><span id=__span-11-5><a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a><span class=c1># CNN: Compute Grad-CAM</span>
</span><span id=__span-11-6><a id=__codelineno-11-6 name=__codelineno-11-6 href=#__codelineno-11-6></a><span class=n>gradcam_heatmap</span> <span class=o>=</span> <span class=n>generate_gradcam</span><span class=p>(</span><span class=n>cnn_model</span><span class=p>,</span> <span class=n>signal</span><span class=p>,</span> <span class=n>label</span><span class=p>)</span>
</span><span id=__span-11-7><a id=__codelineno-11-7 name=__codelineno-11-7 href=#__codelineno-11-7></a>
</span><span id=__span-11-8><a id=__codelineno-11-8 name=__codelineno-11-8 href=#__codelineno-11-8></a><span class=c1># Visualize side-by-side</span>
</span><span id=__span-11-9><a id=__codelineno-11-9 name=__codelineno-11-9 href=#__codelineno-11-9></a><span class=n>fig</span><span class=p>,</span> <span class=p>(</span><span class=n>ax1</span><span class=p>,</span> <span class=n>ax2</span><span class=p>)</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span><span id=__span-11-10><a id=__codelineno-11-10 name=__codelineno-11-10 href=#__codelineno-11-10></a><span class=n>ax1</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>signal</span><span class=p>);</span>  <span class=n>ax1</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>attn_importance</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>  <span class=c1># Attention</span>
</span><span id=__span-11-11><a id=__codelineno-11-11 name=__codelineno-11-11 href=#__codelineno-11-11></a><span class=n>ax2</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>signal</span><span class=p>);</span>  <span class=n>ax2</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>gradcam_heatmap</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>  <span class=c1># Grad-CAM</span>
</span></code></pre></div> - <code>quantify_agreement(attn_map, gradcam_map)</code>: Correlation between methods - <strong>Expected Finding</strong>: Both methods should highlight similar time regions - <strong>Dependencies</strong>: <code>attention_visualization.py</code>, <code>evaluation/cnn_interpretability.py</code></p> <p><strong><code>visualization/attention_dashboard.py</code></strong> - <strong>Purpose</strong>: Interactive dashboard for attention exploration - <strong>Key Classes</strong>: - <code>AttentionDashboard</code>: Streamlit app for attention visualization - <strong>Features</strong>: - Upload signal → see attention heatmap - Slider to select layer (layer 1-6) - Hover over patch → see attention distribution - Compare attention for different fault types - <strong>Dependencies</strong>: <code>streamlit</code>, <code>plotly</code>, <code>attention_visualization.py</code></p> <h4 id=5-hybrid-cnn-transformer-2-files><strong>5. Hybrid CNN-Transformer (2 files)</strong><a class=headerlink href=#5-hybrid-cnn-transformer-2-files title="Permanent link">&para;</a></h4> <p><strong><code>models/hybrid/cnn_transformer.py</code></strong> - <strong>Purpose</strong>: Combine CNN feature extraction with Transformer - <strong>Key Classes</strong>: - <code>CNNTransformer(BaseModel)</code>: CNN backbone + Transformer encoder - <strong>Architecture</strong>: <div class="language-text highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a>Input: [B, 1, 102400]
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a>  ↓
</span><span id=__span-12-3><a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a>CNN Backbone (ResNet-18, remove final FC):
</span><span id=__span-12-4><a id=__codelineno-12-4 name=__codelineno-12-4 href=#__codelineno-12-4></a>  ├─ Conv layers
</span><span id=__span-12-5><a id=__codelineno-12-5 name=__codelineno-12-5 href=#__codelineno-12-5></a>  └─ Output: [B, 512, 800]  # Feature maps
</span><span id=__span-12-6><a id=__codelineno-12-6 name=__codelineno-12-6 href=#__codelineno-12-6></a>  ↓
</span><span id=__span-12-7><a id=__codelineno-12-7 name=__codelineno-12-7 href=#__codelineno-12-7></a>Permute: [B, 512, 800] → [B, 800, 512]  # (batch, seq_len, channels)
</span><span id=__span-12-8><a id=__codelineno-12-8 name=__codelineno-12-8 href=#__codelineno-12-8></a>  ↓
</span><span id=__span-12-9><a id=__codelineno-12-9 name=__codelineno-12-9 href=#__codelineno-12-9></a>Transformer Encoder (4 layers, d_model=512):
</span><span id=__span-12-10><a id=__codelineno-12-10 name=__codelineno-12-10 href=#__codelineno-12-10></a>  ├─ Self-attention over spatial locations
</span><span id=__span-12-11><a id=__codelineno-12-11 name=__codelineno-12-11 href=#__codelineno-12-11></a>  └─ Output: [B, 800, 512]
</span><span id=__span-12-12><a id=__codelineno-12-12 name=__codelineno-12-12 href=#__codelineno-12-12></a>  ↓
</span><span id=__span-12-13><a id=__codelineno-12-13 name=__codelineno-12-13 href=#__codelineno-12-13></a>Global Avg Pool: [B, 800, 512] → [B, 512]
</span><span id=__span-12-14><a id=__codelineno-12-14 name=__codelineno-12-14 href=#__codelineno-12-14></a>  ↓
</span><span id=__span-12-15><a id=__codelineno-12-15 name=__codelineno-12-15 href=#__codelineno-12-15></a>FC: [B, 512] → [B, 11]
</span></code></pre></div> - <strong>Rationale</strong>: - CNN: Inductive bias for local patterns (good for low-level features) - Transformer: Model long-range dependencies (good for high-level reasoning) - Combines strengths of both - <strong>Expected Performance</strong>: 97-98% accuracy (best of both worlds) - <strong>Dependencies</strong>: <code>models/resnet/resnet_1d.py</code>, <code>models/transformer/signal_transformer.py</code></p> <p><strong><code>models/hybrid/perceiver.py</code></strong> - <strong>Purpose</strong>: Perceiver architecture (cross-attention between latents and signal) - <strong>Key Classes</strong>: - <code>Perceiver1D(BaseModel)</code>: Perceiver for signals - <strong>Architecture</strong>: <div class="language-text highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a>Latent array: [n_latents, d_latent]  # e.g., [64, 512]
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a>Input signal patches: [n_patches, d_model]  # [200, 256]
</span><span id=__span-13-3><a id=__codelineno-13-3 name=__codelineno-13-3 href=#__codelineno-13-3></a>
</span><span id=__span-13-4><a id=__codelineno-13-4 name=__codelineno-13-4 href=#__codelineno-13-4></a>Cross-Attention:
</span><span id=__span-13-5><a id=__codelineno-13-5 name=__codelineno-13-5 href=#__codelineno-13-5></a>  Q = Latents
</span><span id=__span-13-6><a id=__codelineno-13-6 name=__codelineno-13-6 href=#__codelineno-13-6></a>  K, V = Input patches
</span><span id=__span-13-7><a id=__codelineno-13-7 name=__codelineno-13-7 href=#__codelineno-13-7></a>  Output: [n_latents, d_latent]  # Bottleneck representation
</span><span id=__span-13-8><a id=__codelineno-13-8 name=__codelineno-13-8 href=#__codelineno-13-8></a>
</span><span id=__span-13-9><a id=__codelineno-13-9 name=__codelineno-13-9 href=#__codelineno-13-9></a>Self-Attention (Transformer):
</span><span id=__span-13-10><a id=__codelineno-13-10 name=__codelineno-13-10 href=#__codelineno-13-10></a>  Process latents: [n_latents, d_latent]
</span><span id=__span-13-11><a id=__codelineno-13-11 name=__codelineno-13-11 href=#__codelineno-13-11></a>
</span><span id=__span-13-12><a id=__codelineno-13-12 name=__codelineno-13-12 href=#__codelineno-13-12></a>Classification:
</span><span id=__span-13-13><a id=__codelineno-13-13 name=__codelineno-13-13 href=#__codelineno-13-13></a>  Pool latents → FC → [11]
</span></code></pre></div> - <strong>Benefit</strong>: O(n_latents × n_patches) instead of O(n_patches²) - <strong>Use Case</strong>: Very long signals (&gt; 100,000 samples) - <strong>Dependencies</strong>: <code>torch.nn</code>, <code>models/base_model.py</code></p> <h4 id=6-evaluation-2-files><strong>6. Evaluation (2 files)</strong><a class=headerlink href=#6-evaluation-2-files title="Permanent link">&para;</a></h4> <p><strong><code>evaluation/transformer_evaluator.py</code></strong> - <strong>Purpose</strong>: Evaluate Transformer models - <strong>Key Classes</strong>: - <code>TransformerEvaluator(ModelEvaluator)</code>: Extends base evaluator - <strong>Additional Metrics</strong>: - Attention entropy: How focused is attention? (low entropy = focused) - Patch importance scores: Which patches are most critical? - Layer-wise attention analysis: How does attention evolve through layers? - <strong>Key Functions</strong>: - <code>evaluate(model, test_loader)</code>: Standard evaluation - <code>analyze_attention_patterns(model, test_loader)</code>: Statistical attention analysis - <strong>Dependencies</strong>: <code>evaluation/evaluator.py</code>, <code>attention_visualization.py</code></p> <p><strong><code>evaluation/transformer_vs_resnet.py</code></strong> - <strong>Purpose</strong>: Systematic comparison Transformer vs. ResNet - <strong>Key Functions</strong>: - <code>compare_architectures(transformer, resnet, test_loader)</code>: <div class="language-python highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=n>results</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a>    <span class=s1>&#39;Accuracy&#39;</span><span class=p>:</span> <span class=p>{</span>
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a>        <span class=s1>&#39;Transformer&#39;</span><span class=p>:</span> <span class=n>transformer_acc</span><span class=p>,</span>
</span><span id=__span-14-4><a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a>        <span class=s1>&#39;ResNet&#39;</span><span class=p>:</span> <span class=n>resnet_acc</span>
</span><span id=__span-14-5><a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a>    <span class=p>},</span>
</span><span id=__span-14-6><a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a>    <span class=s1>&#39;Per-class F1&#39;</span><span class=p>:</span> <span class=p>{</span>
</span><span id=__span-14-7><a id=__codelineno-14-7 name=__codelineno-14-7 href=#__codelineno-14-7></a>        <span class=s1>&#39;Transformer&#39;</span><span class=p>:</span> <span class=n>transformer_f1_per_class</span><span class=p>,</span>
</span><span id=__span-14-8><a id=__codelineno-14-8 name=__codelineno-14-8 href=#__codelineno-14-8></a>        <span class=s1>&#39;ResNet&#39;</span><span class=p>:</span> <span class=n>resnet_f1_per_class</span>
</span><span id=__span-14-9><a id=__codelineno-14-9 name=__codelineno-14-9 href=#__codelineno-14-9></a>    <span class=p>},</span>
</span><span id=__span-14-10><a id=__codelineno-14-10 name=__codelineno-14-10 href=#__codelineno-14-10></a>    <span class=s1>&#39;Inference Time&#39;</span><span class=p>:</span> <span class=p>{</span>
</span><span id=__span-14-11><a id=__codelineno-14-11 name=__codelineno-14-11 href=#__codelineno-14-11></a>        <span class=s1>&#39;Transformer&#39;</span><span class=p>:</span> <span class=n>transformer_time</span><span class=p>,</span>
</span><span id=__span-14-12><a id=__codelineno-14-12 name=__codelineno-14-12 href=#__codelineno-14-12></a>        <span class=s1>&#39;ResNet&#39;</span><span class=p>:</span> <span class=n>resnet_time</span>
</span><span id=__span-14-13><a id=__codelineno-14-13 name=__codelineno-14-13 href=#__codelineno-14-13></a>    <span class=p>},</span>
</span><span id=__span-14-14><a id=__codelineno-14-14 name=__codelineno-14-14 href=#__codelineno-14-14></a>    <span class=s1>&#39;Parameters&#39;</span><span class=p>:</span> <span class=p>{</span>
</span><span id=__span-14-15><a id=__codelineno-14-15 name=__codelineno-14-15 href=#__codelineno-14-15></a>        <span class=s1>&#39;Transformer&#39;</span><span class=p>:</span> <span class=n>count_parameters</span><span class=p>(</span><span class=n>transformer</span><span class=p>),</span>
</span><span id=__span-14-16><a id=__codelineno-14-16 name=__codelineno-14-16 href=#__codelineno-14-16></a>        <span class=s1>&#39;ResNet&#39;</span><span class=p>:</span> <span class=n>count_parameters</span><span class=p>(</span><span class=n>resnet</span><span class=p>)</span>
</span><span id=__span-14-17><a id=__codelineno-14-17 name=__codelineno-14-17 href=#__codelineno-14-17></a>    <span class=p>}</span>
</span><span id=__span-14-18><a id=__codelineno-14-18 name=__codelineno-14-18 href=#__codelineno-14-18></a><span class=p>}</span>
</span><span id=__span-14-19><a id=__codelineno-14-19 name=__codelineno-14-19 href=#__codelineno-14-19></a><span class=k>return</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>results</span><span class=p>)</span>
</span></code></pre></div> - <code>plot_comparison_radar(results)</code>: Radar chart showing trade-offs - <strong>Expected Findings</strong>: - Transformer: Slightly better on mixed faults (long-range dependencies) - ResNet: Slightly faster inference (no attention computation) - Similar accuracy (~0.5% difference) - <strong>Dependencies</strong>: <code>evaluation/evaluator.py</code>, <code>pandas</code>, <code>matplotlib</code></p> <h3 id=architecture-decisions>Architecture Decisions<a class=headerlink href=#architecture-decisions title="Permanent link">&para;</a></h3> <p><strong>1. Patch Size Selection</strong> - <strong>Decision</strong>: Use patch_size=512 (non-overlapping) - <strong>Rationale</strong>: - 102,400 samples / 512 = 200 patches (manageable sequence length) - Each patch covers ~25ms of signal at 20.48 kHz (captures fault cycles) - Non-overlapping avoids redundancy, faster training - <strong>Alternative</strong>: Overlapping patches (stride=256) for 400 patches (more detail, slower)</p> <p><strong>2. Positional Encoding: Learnable vs. Sinusoidal</strong> - <strong>Decision</strong>: Use learnable positional embeddings - <strong>Rationale</strong>: - Signals have irregular temporal patterns (not natural language) - Learnable embeddings can adapt to fault-specific patterns - Minimal parameter overhead (200 patches × 256 dim = 51k params) - <strong>Fallback</strong>: Sinusoidal if overfitting occurs</p> <p><strong>3. Number of Transformer Layers</strong> - <strong>Decision</strong>: 6 layers (standard Transformer depth) - <strong>Rationale</strong>: - Deeper than 6 layers risks overfitting on 1,430 samples - Shallower than 6 layers may not capture complex patterns - 6 layers proven effective in ViT, BERT - <strong>Experiment</strong>: Try 4, 6, 8 layers, compare validation accuracy</p> <p><strong>4. Multi-Head Attention: 8 Heads</strong> - <strong>Decision</strong>: Use 8 attention heads - <strong>Rationale</strong>: - d_model=256 / 8 heads = 32 dim per head (not too small) - Multiple heads capture different temporal relationships - Standard in Transformer literature - <strong>Constraint</strong>: n_heads must divide d_model evenly</p> <p><strong>5. Classification Token vs. Global Average Pooling</strong> - <strong>Decision</strong>: Start with global average pooling, experiment with [CLS] token - <strong>Rationale</strong>: - Global avg pool is simpler, fewer hyperparameters - [CLS] token (ViT-style) may be better but requires tuning - Easy to swap implementations, test both - <strong>Evaluation</strong>: Compare accuracy, use better one</p> <p><strong>6. Warmup Schedule (Critical for Transformers)</strong> - <strong>Decision</strong>: 5-epoch linear warmup, then cosine annealing - <strong>Rationale</strong>: - Transformers sensitive to learning rate at start of training - Without warmup: training diverges or converges slowly - Warmup stabilizes gradients in early epochs - <strong>Evidence</strong>: Required in original "Attention is All You Need" paper</p> <h3 id=data-flow>Data Flow<a class=headerlink href=#data-flow title="Permanent link">&para;</a></h3> <div class="language-text highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a>┌────────────────────────────────────────────────────────────┐
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a>│         TRANSFORMER TRAINING PIPELINE (Phase 4)             │
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a>└────────────────────────────────────────────────────────────┘
</span><span id=__span-15-4><a id=__codelineno-15-4 name=__codelineno-15-4 href=#__codelineno-15-4></a>
</span><span id=__span-15-5><a id=__codelineno-15-5 name=__codelineno-15-5 href=#__codelineno-15-5></a>1. DATA LOADING (same as Phases 2-3)
</span><span id=__span-15-6><a id=__codelineno-15-6 name=__codelineno-15-6 href=#__codelineno-15-6></a>   ┌──────────────────────────────────────────────────────┐
</span><span id=__span-15-7><a id=__codelineno-15-7 name=__codelineno-15-7 href=#__codelineno-15-7></a>   │ data/cnn_dataloader.py                                │
</span><span id=__span-15-8><a id=__codelineno-15-8 name=__codelineno-15-8 href=#__codelineno-15-8></a>   │  └─ Load raw signals [B, 1, 102400]                  │
</span><span id=__span-15-9><a id=__codelineno-15-9 name=__codelineno-15-9 href=#__codelineno-15-9></a>   └──────────────────────────────────────────────────────┘
</span><span id=__span-15-10><a id=__codelineno-15-10 name=__codelineno-15-10 href=#__codelineno-15-10></a>                        ↓
</span><span id=__span-15-11><a id=__codelineno-15-11 name=__codelineno-15-11 href=#__codelineno-15-11></a>
</span><span id=__span-15-12><a id=__codelineno-15-12 name=__codelineno-15-12 href=#__codelineno-15-12></a>2. PATCH EMBEDDING
</span><span id=__span-15-13><a id=__codelineno-15-13 name=__codelineno-15-13 href=#__codelineno-15-13></a>   ┌──────────────────────────────────────────────────────┐
</span><span id=__span-15-14><a id=__codelineno-15-14 name=__codelineno-15-14 href=#__codelineno-15-14></a>   │ models/transformer/patch_embedding.py                 │
</span><span id=__span-15-15><a id=__codelineno-15-15 name=__codelineno-15-15 href=#__codelineno-15-15></a>   │                                                       │
</span><span id=__span-15-16><a id=__codelineno-15-16 name=__codelineno-15-16 href=#__codelineno-15-16></a>   │ Input: [B, 1, 102400]                                 │
</span><span id=__span-15-17><a id=__codelineno-15-17 name=__codelineno-15-17 href=#__codelineno-15-17></a>   │  ↓                                                    │
</span><span id=__span-15-18><a id=__codelineno-15-18 name=__codelineno-15-18 href=#__codelineno-15-18></a>   │ Reshape: [B, 200, 512]  # 200 patches, 512 samples each
</span><span id=__span-15-19><a id=__codelineno-15-19 name=__codelineno-15-19 href=#__codelineno-15-19></a>   │  ↓                                                    │
</span><span id=__span-15-20><a id=__codelineno-15-20 name=__codelineno-15-20 href=#__codelineno-15-20></a>   │ Linear projection: [B, 200, 512] → [B, 200, 256]    │
</span><span id=__span-15-21><a id=__codelineno-15-21 name=__codelineno-15-21 href=#__codelineno-15-21></a>   │         (d_model=256)                                 │
</span><span id=__span-15-22><a id=__codelineno-15-22 name=__codelineno-15-22 href=#__codelineno-15-22></a>   │  ↓                                                    │
</span><span id=__span-15-23><a id=__codelineno-15-23 name=__codelineno-15-23 href=#__codelineno-15-23></a>   │ Add positional encoding: [B, 200, 256]               │
</span><span id=__span-15-24><a id=__codelineno-15-24 name=__codelineno-15-24 href=#__codelineno-15-24></a>   │         (learnable embeddings)                        │
</span><span id=__span-15-25><a id=__codelineno-15-25 name=__codelineno-15-25 href=#__codelineno-15-25></a>   └──────────────────────────────────────────────────────┘
</span><span id=__span-15-26><a id=__codelineno-15-26 name=__codelineno-15-26 href=#__codelineno-15-26></a>                        ↓
</span><span id=__span-15-27><a id=__codelineno-15-27 name=__codelineno-15-27 href=#__codelineno-15-27></a>
</span><span id=__span-15-28><a id=__codelineno-15-28 name=__codelineno-15-28 href=#__codelineno-15-28></a>3. TRANSFORMER ENCODER (6 layers)
</span><span id=__span-15-29><a id=__codelineno-15-29 name=__codelineno-15-29 href=#__codelineno-15-29></a>   ┌──────────────────────────────────────────────────────┐
</span><span id=__span-15-30><a id=__codelineno-15-30 name=__codelineno-15-30 href=#__codelineno-15-30></a>   │ models/transformer/signal_transformer.py              │
</span><span id=__span-15-31><a id=__codelineno-15-31 name=__codelineno-15-31 href=#__codelineno-15-31></a>   │                                                       │
</span><span id=__span-15-32><a id=__codelineno-15-32 name=__codelineno-15-32 href=#__codelineno-15-32></a>   │ For layer in [1..6]:                                  │
</span><span id=__span-15-33><a id=__codelineno-15-33 name=__codelineno-15-33 href=#__codelineno-15-33></a>   │   ┌────────────────────────────────────────────────┐ │
</span><span id=__span-15-34><a id=__codelineno-15-34 name=__codelineno-15-34 href=#__codelineno-15-34></a>   │   │ Multi-Head Self-Attention (8 heads):          │ │
</span><span id=__span-15-35><a id=__codelineno-15-35 name=__codelineno-15-35 href=#__codelineno-15-35></a>   │   │  ├─ Q, K, V = Linear(x)                       │ │
</span><span id=__span-15-36><a id=__codelineno-15-36 name=__codelineno-15-36 href=#__codelineno-15-36></a>   │   │  ├─ Attention = softmax(QK^T / √32) V        │ │
</span><span id=__span-15-37><a id=__codelineno-15-37 name=__codelineno-15-37 href=#__codelineno-15-37></a>   │   │  └─ Output: [B, 200, 256]                     │ │
</span><span id=__span-15-38><a id=__codelineno-15-38 name=__codelineno-15-38 href=#__codelineno-15-38></a>   │   │                                                 │ │
</span><span id=__span-15-39><a id=__codelineno-15-39 name=__codelineno-15-39 href=#__codelineno-15-39></a>   │   │ Add &amp; Norm: x + Attention(x), LayerNorm       │ │
</span><span id=__span-15-40><a id=__codelineno-15-40 name=__codelineno-15-40 href=#__codelineno-15-40></a>   │   │                                                 │ │
</span><span id=__span-15-41><a id=__codelineno-15-41 name=__codelineno-15-41 href=#__codelineno-15-41></a>   │   │ Feed-Forward:                                  │ │
</span><span id=__span-15-42><a id=__codelineno-15-42 name=__codelineno-15-42 href=#__codelineno-15-42></a>   │   │  ├─ FC: 256 → 1024 → 256                     │ │
</span><span id=__span-15-43><a id=__codelineno-15-43 name=__codelineno-15-43 href=#__codelineno-15-43></a>   │   │  └─ GELU, Dropout                             │ │
</span><span id=__span-15-44><a id=__codelineno-15-44 name=__codelineno-15-44 href=#__codelineno-15-44></a>   │   │                                                 │ │
</span><span id=__span-15-45><a id=__codelineno-15-45 name=__codelineno-15-45 href=#__codelineno-15-45></a>   │   │ Add &amp; Norm: x + FFN(x), LayerNorm             │ │
</span><span id=__span-15-46><a id=__codelineno-15-46 name=__codelineno-15-46 href=#__codelineno-15-46></a>   │   └────────────────────────────────────────────────┘ │
</span><span id=__span-15-47><a id=__codelineno-15-47 name=__codelineno-15-47 href=#__codelineno-15-47></a>   │ Output after 6 layers: [B, 200, 256]                 │
</span><span id=__span-15-48><a id=__codelineno-15-48 name=__codelineno-15-48 href=#__codelineno-15-48></a>   └──────────────────────────────────────────────────────┘
</span><span id=__span-15-49><a id=__codelineno-15-49 name=__codelineno-15-49 href=#__codelineno-15-49></a>                        ↓
</span><span id=__span-15-50><a id=__codelineno-15-50 name=__codelineno-15-50 href=#__codelineno-15-50></a>
</span><span id=__span-15-51><a id=__codelineno-15-51 name=__codelineno-15-51 href=#__codelineno-15-51></a>4. CLASSIFICATION HEAD
</span><span id=__span-15-52><a id=__codelineno-15-52 name=__codelineno-15-52 href=#__codelineno-15-52></a>   ┌──────────────────────────────────────────────────────┐
</span><span id=__span-15-53><a id=__codelineno-15-53 name=__codelineno-15-53 href=#__codelineno-15-53></a>   │ Global Average Pool:                                  │
</span><span id=__span-15-54><a id=__codelineno-15-54 name=__codelineno-15-54 href=#__codelineno-15-54></a>   │   [B, 200, 256] → [B, 256]  # Average over patches  │
</span><span id=__span-15-55><a id=__codelineno-15-55 name=__codelineno-15-55 href=#__codelineno-15-55></a>   │         ↓                                             │
</span><span id=__span-15-56><a id=__codelineno-15-56 name=__codelineno-15-56 href=#__codelineno-15-56></a>   │ Fully Connected:                                      │
</span><span id=__span-15-57><a id=__codelineno-15-57 name=__codelineno-15-57 href=#__codelineno-15-57></a>   │   [B, 256] → [B, 11]  # Class logits                 │
</span><span id=__span-15-58><a id=__codelineno-15-58 name=__codelineno-15-58 href=#__codelineno-15-58></a>   └──────────────────────────────────────────────────────┘
</span><span id=__span-15-59><a id=__codelineno-15-59 name=__codelineno-15-59 href=#__codelineno-15-59></a>                        ↓
</span><span id=__span-15-60><a id=__codelineno-15-60 name=__codelineno-15-60 href=#__codelineno-15-60></a>
</span><span id=__span-15-61><a id=__codelineno-15-61 name=__codelineno-15-61 href=#__codelineno-15-61></a>5. ATTENTION VISUALIZATION (during evaluation)
</span><span id=__span-15-62><a id=__codelineno-15-62 name=__codelineno-15-62 href=#__codelineno-15-62></a>   ┌──────────────────────────────────────────────────────┐
</span><span id=__span-15-63><a id=__codelineno-15-63 name=__codelineno-15-63 href=#__codelineno-15-63></a>   │ evaluation/attention_visualization.py                 │
</span><span id=__span-15-64><a id=__codelineno-15-64 name=__codelineno-15-64 href=#__codelineno-15-64></a>   │                                                       │
</span><span id=__span-15-65><a id=__codelineno-15-65 name=__codelineno-15-65 href=#__codelineno-15-65></a>   │ Extract attention weights from each layer:            │
</span><span id=__span-15-66><a id=__codelineno-15-66 name=__codelineno-15-66 href=#__codelineno-15-66></a>   │   attention_weights: [B, n_heads, n_patches, n_patches]
</span><span id=__span-15-67><a id=__codelineno-15-67 name=__codelineno-15-67 href=#__codelineno-15-67></a>   │         ↓                                             │
</span><span id=__span-15-68><a id=__codelineno-15-68 name=__codelineno-15-68 href=#__codelineno-15-68></a>   │ Average over heads: [B, n_patches, n_patches]        │
</span><span id=__span-15-69><a id=__codelineno-15-69 name=__codelineno-15-69 href=#__codelineno-15-69></a>   │         ↓                                             │
</span><span id=__span-15-70><a id=__codelineno-15-70 name=__codelineno-15-70 href=#__codelineno-15-70></a>   │ Visualize as heatmap:                                 │
</span><span id=__span-15-71><a id=__codelineno-15-71 name=__codelineno-15-71 href=#__codelineno-15-71></a>   │   - Rows: Query patches                               │
</span><span id=__span-15-72><a id=__codelineno-15-72 name=__codelineno-15-72 href=#__codelineno-15-72></a>   │   - Columns: Key patches                              │
</span><span id=__span-15-73><a id=__codelineno-15-73 name=__codelineno-15-73 href=#__codelineno-15-73></a>   │   - Color: Attention score                            │
</span><span id=__span-15-74><a id=__codelineno-15-74 name=__codelineno-15-74 href=#__codelineno-15-74></a>   │         ↓                                             │
</span><span id=__span-15-75><a id=__codelineno-15-75 name=__codelineno-15-75 href=#__codelineno-15-75></a>   │ Interpret: Which time regions are important for       │
</span><span id=__span-15-76><a id=__codelineno-15-76 name=__codelineno-15-76 href=#__codelineno-15-76></a>   │            classifying this signal?                   │
</span><span id=__span-15-77><a id=__codelineno-15-77 name=__codelineno-15-77 href=#__codelineno-15-77></a>   └──────────────────────────────────────────────────────┘
</span></code></pre></div> <h3 id=integration-points>Integration Points<a class=headerlink href=#integration-points title="Permanent link">&para;</a></h3> <p><strong>1. With Phase 3 (ResNet)</strong> - <strong>Comparison</strong>: Transformer vs. ResNet-50 accuracy, interpretability - <strong>Ensemble</strong>: Combine Transformer + ResNet predictions (Phase 8) - <strong>Hybrid</strong>: Use ResNet as CNN backbone in CNN-Transformer model</p> <p><strong>2. With Phase 2 (Baseline CNN)</strong> - <strong>Progression</strong>: Plain CNN → ResNet → Transformer (increasing complexity) - <strong>Benchmark</strong>: Transformer should match ResNet-50 (96-97% accuracy)</p> <p><strong>3. With Phase 5 (Time-Frequency Analysis)</strong> - <strong>Input</strong>: Transformer can process spectrogram patches (2D patches flattened to 1D) - <strong>Comparison</strong>: Raw signal Transformer vs. Spectrogram Transformer</p> <p><strong>4. With Phase 6 (Physics-Informed)</strong> - <strong>Attention as Physics</strong>: Attention weights can be constrained to focus on fault-relevant frequencies - <strong>Hybrid</strong>: Physics features + Transformer learned features</p> <p><strong>5. With Phase 7 (XAI)</strong> - <strong>Interpretability</strong>: Attention visualization is core XAI method - <strong>Comparison</strong>: Attention vs. SHAP vs. Grad-CAM</p> <h3 id=testing-strategy>Testing Strategy<a class=headerlink href=#testing-strategy title="Permanent link">&para;</a></h3> <p><strong>1. Unit Tests</strong></p> <p><strong><code>tests/test_transformer.py</code></strong> <div class="language-python highlight"><pre><span></span><code><span id=__span-16-1><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a><span class=k>def</span><span class=w> </span><span class=nf>test_patch_embedding</span><span class=p>():</span>
</span><span id=__span-16-2><a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Test patch embedding converts signal to patches correctly.&quot;&quot;&quot;</span>
</span><span id=__span-16-3><a id=__codelineno-16-3 name=__codelineno-16-3 href=#__codelineno-16-3></a>    <span class=n>patch_emb</span> <span class=o>=</span> <span class=n>PatchEmbedding1D</span><span class=p>(</span><span class=n>patch_size</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>d_model</span><span class=o>=</span><span class=mi>256</span><span class=p>)</span>
</span><span id=__span-16-4><a id=__codelineno-16-4 name=__codelineno-16-4 href=#__codelineno-16-4></a>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>102400</span><span class=p>)</span>
</span><span id=__span-16-5><a id=__codelineno-16-5 name=__codelineno-16-5 href=#__codelineno-16-5></a>    <span class=n>patches</span> <span class=o>=</span> <span class=n>patch_emb</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-16-6><a id=__codelineno-16-6 name=__codelineno-16-6 href=#__codelineno-16-6></a>    <span class=k>assert</span> <span class=n>patches</span><span class=o>.</span><span class=n>shape</span> <span class=o>==</span> <span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>  <span class=c1># 200 patches, 256-dim embeddings</span>
</span><span id=__span-16-7><a id=__codelineno-16-7 name=__codelineno-16-7 href=#__codelineno-16-7></a>
</span><span id=__span-16-8><a id=__codelineno-16-8 name=__codelineno-16-8 href=#__codelineno-16-8></a><span class=k>def</span><span class=w> </span><span class=nf>test_multi_head_attention</span><span class=p>():</span>
</span><span id=__span-16-9><a id=__codelineno-16-9 name=__codelineno-16-9 href=#__codelineno-16-9></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Test multi-head attention output shape.&quot;&quot;&quot;</span>
</span><span id=__span-16-10><a id=__codelineno-16-10 name=__codelineno-16-10 href=#__codelineno-16-10></a>    <span class=n>attn</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=o>=</span><span class=mi>256</span><span class=p>,</span> <span class=n>n_heads</span><span class=o>=</span><span class=mi>8</span><span class=p>)</span>
</span><span id=__span-16-11><a id=__codelineno-16-11 name=__codelineno-16-11 href=#__codelineno-16-11></a>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>  <span class=c1># (batch, seq_len, d_model)</span>
</span><span id=__span-16-12><a id=__codelineno-16-12 name=__codelineno-16-12 href=#__codelineno-16-12></a>    <span class=n>output</span><span class=p>,</span> <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>attn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-16-13><a id=__codelineno-16-13 name=__codelineno-16-13 href=#__codelineno-16-13></a>    <span class=k>assert</span> <span class=n>output</span><span class=o>.</span><span class=n>shape</span> <span class=o>==</span> <span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>
</span><span id=__span-16-14><a id=__codelineno-16-14 name=__codelineno-16-14 href=#__codelineno-16-14></a>    <span class=k>assert</span> <span class=n>attn_weights</span><span class=o>.</span><span class=n>shape</span> <span class=o>==</span> <span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>200</span><span class=p>,</span> <span class=mi>200</span><span class=p>)</span>  <span class=c1># (batch, heads, seq_len, seq_len)</span>
</span><span id=__span-16-15><a id=__codelineno-16-15 name=__codelineno-16-15 href=#__codelineno-16-15></a>
</span><span id=__span-16-16><a id=__codelineno-16-16 name=__codelineno-16-16 href=#__codelineno-16-16></a><span class=k>def</span><span class=w> </span><span class=nf>test_transformer_forward</span><span class=p>():</span>
</span><span id=__span-16-17><a id=__codelineno-16-17 name=__codelineno-16-17 href=#__codelineno-16-17></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Test full Transformer forward pass.&quot;&quot;&quot;</span>
</span><span id=__span-16-18><a id=__codelineno-16-18 name=__codelineno-16-18 href=#__codelineno-16-18></a>    <span class=n>model</span> <span class=o>=</span> <span class=n>SignalTransformer</span><span class=p>(</span><span class=n>d_model</span><span class=o>=</span><span class=mi>256</span><span class=p>,</span> <span class=n>n_heads</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>n_layers</span><span class=o>=</span><span class=mi>6</span><span class=p>)</span>
</span><span id=__span-16-19><a id=__codelineno-16-19 name=__codelineno-16-19 href=#__codelineno-16-19></a>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>102400</span><span class=p>)</span>
</span><span id=__span-16-20><a id=__codelineno-16-20 name=__codelineno-16-20 href=#__codelineno-16-20></a>    <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-16-21><a id=__codelineno-16-21 name=__codelineno-16-21 href=#__codelineno-16-21></a>    <span class=k>assert</span> <span class=n>output</span><span class=o>.</span><span class=n>shape</span> <span class=o>==</span> <span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>11</span><span class=p>)</span>  <span class=c1># 11 classes</span>
</span></code></pre></div></p> <p><strong>2. Attention Validation Tests</strong></p> <p><strong><code>tests/test_attention_visualization.py</code></strong> <div class="language-python highlight"><pre><span></span><code><span id=__span-17-1><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a><span class=k>def</span><span class=w> </span><span class=nf>test_attention_weights_sum_to_one</span><span class=p>():</span>
</span><span id=__span-17-2><a id=__codelineno-17-2 name=__codelineno-17-2 href=#__codelineno-17-2></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Attention weights should sum to 1 (valid probability distribution).&quot;&quot;&quot;</span>
</span><span id=__span-17-3><a id=__codelineno-17-3 name=__codelineno-17-3 href=#__codelineno-17-3></a>    <span class=n>model</span> <span class=o>=</span> <span class=n>SignalTransformer</span><span class=p>()</span>
</span><span id=__span-17-4><a id=__codelineno-17-4 name=__codelineno-17-4 href=#__codelineno-17-4></a>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>102400</span><span class=p>)</span>
</span><span id=__span-17-5><a id=__codelineno-17-5 name=__codelineno-17-5 href=#__codelineno-17-5></a>    <span class=n>_</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-17-6><a id=__codelineno-17-6 name=__codelineno-17-6 href=#__codelineno-17-6></a>    <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>get_attention_weights</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>layer_idx</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span><span id=__span-17-7><a id=__codelineno-17-7 name=__codelineno-17-7 href=#__codelineno-17-7></a>
</span><span id=__span-17-8><a id=__codelineno-17-8 name=__codelineno-17-8 href=#__codelineno-17-8></a>    <span class=c1># Sum over key dimension should be 1</span>
</span><span id=__span-17-9><a id=__codelineno-17-9 name=__codelineno-17-9 href=#__codelineno-17-9></a>    <span class=n>attn_sum</span> <span class=o>=</span> <span class=n>attn_weights</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-17-10><a id=__codelineno-17-10 name=__codelineno-17-10 href=#__codelineno-17-10></a>    <span class=n>torch</span><span class=o>.</span><span class=n>testing</span><span class=o>.</span><span class=n>assert_allclose</span><span class=p>(</span><span class=n>attn_sum</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones_like</span><span class=p>(</span><span class=n>attn_sum</span><span class=p>),</span> <span class=n>rtol</span><span class=o>=</span><span class=mf>1e-5</span><span class=p>)</span>
</span><span id=__span-17-11><a id=__codelineno-17-11 name=__codelineno-17-11 href=#__codelineno-17-11></a>
</span><span id=__span-17-12><a id=__codelineno-17-12 name=__codelineno-17-12 href=#__codelineno-17-12></a><span class=k>def</span><span class=w> </span><span class=nf>test_attention_focuses_on_relevant_regions</span><span class=p>():</span>
</span><span id=__span-17-13><a id=__codelineno-17-13 name=__codelineno-17-13 href=#__codelineno-17-13></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;For known fault signal, attention should focus on expected regions.&quot;&quot;&quot;</span>
</span><span id=__span-17-14><a id=__codelineno-17-14 name=__codelineno-17-14 href=#__codelineno-17-14></a>    <span class=c1># Load misalignment signal (known to have strong 2X harmonic)</span>
</span><span id=__span-17-15><a id=__codelineno-17-15 name=__codelineno-17-15 href=#__codelineno-17-15></a>    <span class=n>signal</span> <span class=o>=</span> <span class=n>load_test_signal</span><span class=p>(</span><span class=s1>&#39;misalignment_moderate.mat&#39;</span><span class=p>)</span>
</span><span id=__span-17-16><a id=__codelineno-17-16 name=__codelineno-17-16 href=#__codelineno-17-16></a>    <span class=n>model</span> <span class=o>=</span> <span class=n>load_trained_transformer</span><span class=p>()</span>
</span><span id=__span-17-17><a id=__codelineno-17-17 name=__codelineno-17-17 href=#__codelineno-17-17></a>
</span><span id=__span-17-18><a id=__codelineno-17-18 name=__codelineno-17-18 href=#__codelineno-17-18></a>    <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>get_attention_weights</span><span class=p>(</span><span class=n>signal</span><span class=p>,</span> <span class=n>layer_idx</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span><span id=__span-17-19><a id=__codelineno-17-19 name=__codelineno-17-19 href=#__codelineno-17-19></a>    <span class=n>important_patches</span> <span class=o>=</span> <span class=n>find_most_attended_patches</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>top_k</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span><span id=__span-17-20><a id=__codelineno-17-20 name=__codelineno-17-20 href=#__codelineno-17-20></a>
</span><span id=__span-17-21><a id=__codelineno-17-21 name=__codelineno-17-21 href=#__codelineno-17-21></a>    <span class=c1># Check if important patches correspond to 2X harmonic regions</span>
</span><span id=__span-17-22><a id=__codelineno-17-22 name=__codelineno-17-22 href=#__codelineno-17-22></a>    <span class=c1># (Requires domain knowledge of where 2X harmonic appears in time domain)</span>
</span><span id=__span-17-23><a id=__codelineno-17-23 name=__codelineno-17-23 href=#__codelineno-17-23></a>    <span class=c1># This is a qualitative test - manual inspection required</span>
</span><span id=__span-17-24><a id=__codelineno-17-24 name=__codelineno-17-24 href=#__codelineno-17-24></a>    <span class=n>plot_signal_with_attention</span><span class=p>(</span><span class=n>signal</span><span class=p>,</span> <span class=n>important_patches</span><span class=p>)</span>
</span></code></pre></div></p> <p><strong>3. Comparison Tests</strong></p> <p><strong><code>tests/test_transformer_vs_resnet.py</code></strong> <div class="language-python highlight"><pre><span></span><code><span id=__span-18-1><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a><span class=k>def</span><span class=w> </span><span class=nf>test_transformer_matches_resnet_accuracy</span><span class=p>():</span>
</span><span id=__span-18-2><a id=__codelineno-18-2 name=__codelineno-18-2 href=#__codelineno-18-2></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Transformer should achieve similar accuracy to ResNet-50.&quot;&quot;&quot;</span>
</span><span id=__span-18-3><a id=__codelineno-18-3 name=__codelineno-18-3 href=#__codelineno-18-3></a>    <span class=n>test_loader</span> <span class=o>=</span> <span class=n>load_standard_test_set</span><span class=p>()</span>
</span><span id=__span-18-4><a id=__codelineno-18-4 name=__codelineno-18-4 href=#__codelineno-18-4></a>
</span><span id=__span-18-5><a id=__codelineno-18-5 name=__codelineno-18-5 href=#__codelineno-18-5></a>    <span class=n>transformer</span> <span class=o>=</span> <span class=n>load_trained_model</span><span class=p>(</span><span class=s1>&#39;SignalTransformer&#39;</span><span class=p>)</span>
</span><span id=__span-18-6><a id=__codelineno-18-6 name=__codelineno-18-6 href=#__codelineno-18-6></a>    <span class=n>resnet50</span> <span class=o>=</span> <span class=n>load_trained_model</span><span class=p>(</span><span class=s1>&#39;ResNet50_1D&#39;</span><span class=p>)</span>
</span><span id=__span-18-7><a id=__codelineno-18-7 name=__codelineno-18-7 href=#__codelineno-18-7></a>
</span><span id=__span-18-8><a id=__codelineno-18-8 name=__codelineno-18-8 href=#__codelineno-18-8></a>    <span class=n>transformer_acc</span> <span class=o>=</span> <span class=n>evaluate</span><span class=p>(</span><span class=n>transformer</span><span class=p>,</span> <span class=n>test_loader</span><span class=p>)</span>
</span><span id=__span-18-9><a id=__codelineno-18-9 name=__codelineno-18-9 href=#__codelineno-18-9></a>    <span class=n>resnet_acc</span> <span class=o>=</span> <span class=n>evaluate</span><span class=p>(</span><span class=n>resnet50</span><span class=p>,</span> <span class=n>test_loader</span><span class=p>)</span>
</span><span id=__span-18-10><a id=__codelineno-18-10 name=__codelineno-18-10 href=#__codelineno-18-10></a>
</span><span id=__span-18-11><a id=__codelineno-18-11 name=__codelineno-18-11 href=#__codelineno-18-11></a>    <span class=c1># Allow 2% difference</span>
</span><span id=__span-18-12><a id=__codelineno-18-12 name=__codelineno-18-12 href=#__codelineno-18-12></a>    <span class=k>assert</span> <span class=nb>abs</span><span class=p>(</span><span class=n>transformer_acc</span> <span class=o>-</span> <span class=n>resnet_acc</span><span class=p>)</span> <span class=o>&lt;</span> <span class=mf>0.02</span><span class=p>,</span> \
</span><span id=__span-18-13><a id=__codelineno-18-13 name=__codelineno-18-13 href=#__codelineno-18-13></a>        <span class=sa>f</span><span class=s2>&quot;Transformer (</span><span class=si>{</span><span class=n>transformer_acc</span><span class=si>:</span><span class=s2>.2%</span><span class=si>}</span><span class=s2>) vs ResNet (</span><span class=si>{</span><span class=n>resnet_acc</span><span class=si>:</span><span class=s2>.2%</span><span class=si>}</span><span class=s2>) differ by &gt; 2%&quot;</span>
</span></code></pre></div></p> <p><strong>4. Interpretability Tests</strong></p> <p><strong><code>tests/test_interpretability_agreement.py</code></strong> <div class="language-python highlight"><pre><span></span><code><span id=__span-19-1><a id=__codelineno-19-1 name=__codelineno-19-1 href=#__codelineno-19-1></a><span class=k>def</span><span class=w> </span><span class=nf>test_attention_and_gradcam_agree</span><span class=p>():</span>
</span><span id=__span-19-2><a id=__codelineno-19-2 name=__codelineno-19-2 href=#__codelineno-19-2></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Attention and Grad-CAM should highlight similar regions.&quot;&quot;&quot;</span>
</span><span id=__span-19-3><a id=__codelineno-19-3 name=__codelineno-19-3 href=#__codelineno-19-3></a>    <span class=n>signal</span> <span class=o>=</span> <span class=n>load_test_signal</span><span class=p>(</span><span class=s1>&#39;oil_whirl_severe.mat&#39;</span><span class=p>)</span>
</span><span id=__span-19-4><a id=__codelineno-19-4 name=__codelineno-19-4 href=#__codelineno-19-4></a>    <span class=n>transformer</span> <span class=o>=</span> <span class=n>load_trained_transformer</span><span class=p>()</span>
</span><span id=__span-19-5><a id=__codelineno-19-5 name=__codelineno-19-5 href=#__codelineno-19-5></a>    <span class=n>resnet</span> <span class=o>=</span> <span class=n>load_trained_resnet</span><span class=p>()</span>
</span><span id=__span-19-6><a id=__codelineno-19-6 name=__codelineno-19-6 href=#__codelineno-19-6></a>
</span><span id=__span-19-7><a id=__codelineno-19-7 name=__codelineno-19-7 href=#__codelineno-19-7></a>    <span class=c1># Get attention map</span>
</span><span id=__span-19-8><a id=__codelineno-19-8 name=__codelineno-19-8 href=#__codelineno-19-8></a>    <span class=n>attn_map</span> <span class=o>=</span> <span class=n>get_attention_importance</span><span class=p>(</span><span class=n>transformer</span><span class=p>,</span> <span class=n>signal</span><span class=p>)</span>
</span><span id=__span-19-9><a id=__codelineno-19-9 name=__codelineno-19-9 href=#__codelineno-19-9></a>
</span><span id=__span-19-10><a id=__codelineno-19-10 name=__codelineno-19-10 href=#__codelineno-19-10></a>    <span class=c1># Get Grad-CAM</span>
</span><span id=__span-19-11><a id=__codelineno-19-11 name=__codelineno-19-11 href=#__codelineno-19-11></a>    <span class=n>gradcam_map</span> <span class=o>=</span> <span class=n>generate_gradcam</span><span class=p>(</span><span class=n>resnet</span><span class=p>,</span> <span class=n>signal</span><span class=p>,</span> <span class=n>target_class</span><span class=o>=</span><span class=s1>&#39;oilwhirl&#39;</span><span class=p>)</span>
</span><span id=__span-19-12><a id=__codelineno-19-12 name=__codelineno-19-12 href=#__codelineno-19-12></a>
</span><span id=__span-19-13><a id=__codelineno-19-13 name=__codelineno-19-13 href=#__codelineno-19-13></a>    <span class=c1># Compute correlation</span>
</span><span id=__span-19-14><a id=__codelineno-19-14 name=__codelineno-19-14 href=#__codelineno-19-14></a>    <span class=n>correlation</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>corrcoef</span><span class=p>(</span><span class=n>attn_map</span><span class=o>.</span><span class=n>flatten</span><span class=p>(),</span> <span class=n>gradcam_map</span><span class=o>.</span><span class=n>flatten</span><span class=p>())[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>
</span><span id=__span-19-15><a id=__codelineno-19-15 name=__codelineno-19-15 href=#__codelineno-19-15></a>
</span><span id=__span-19-16><a id=__codelineno-19-16 name=__codelineno-19-16 href=#__codelineno-19-16></a>    <span class=c1># Should be moderately correlated (&gt; 0.5)</span>
</span><span id=__span-19-17><a id=__codelineno-19-17 name=__codelineno-19-17 href=#__codelineno-19-17></a>    <span class=k>assert</span> <span class=n>correlation</span> <span class=o>&gt;</span> <span class=mf>0.5</span><span class=p>,</span> \
</span><span id=__span-19-18><a id=__codelineno-19-18 name=__codelineno-19-18 href=#__codelineno-19-18></a>        <span class=sa>f</span><span class=s2>&quot;Attention and Grad-CAM poorly correlated (</span><span class=si>{</span><span class=n>correlation</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>)&quot;</span>
</span></code></pre></div></p> <h3 id=acceptance-criteria>Acceptance Criteria<a class=headerlink href=#acceptance-criteria title="Permanent link">&para;</a></h3> <p><strong>Phase 4 Complete When:</strong></p> <p>✅ <strong>Transformer model trains successfully</strong> - Forward pass completes without errors - Attention weights sum to 1 (valid probability distribution) - Model converges on training set (&gt; 95% train accuracy) - Warmup schedule prevents training divergence</p> <p>✅ <strong>Achieves target accuracy</strong> - <strong>Transformer</strong>: 96-97% test accuracy (matches ResNet-50) - <strong>CNN-Transformer hybrid</strong>: 97-98% test accuracy (best overall) - <strong>Per-class recall</strong>: ≥ 85% for at least 10/11 classes</p> <p>✅ <strong>Attention visualization functional</strong> - Can extract attention weights from any layer - Heatmap visualization implemented - Attention rollout (aggregate across layers) working - Dashboard for interactive exploration</p> <p>✅ <strong>Interpretability validated</strong> - Attention focuses on fault-relevant time regions (qualitative check) - For misalignment: Attention highlights 2X harmonic regions - For oil whirl: Attention highlights sub-synchronous regions (0.42-0.48X) - Attention and Grad-CAM show moderate agreement (correlation &gt; 0.5)</p> <p>✅ <strong>Comparison with ResNet documented</strong> - Side-by-side accuracy table - Inference time comparison (Transformer likely slower due to attention) - Interpretability comparison (attention vs. Grad-CAM) - Error analysis: Does Transformer handle different faults better?</p> <p>✅ <strong>Transformer variants explored</strong> - ViT (with [CLS] token) vs. standard Transformer tested - Performer (efficient attention) achieves similar accuracy with faster training - CNN-Transformer hybrid outperforms pure Transformer</p> <p>✅ <strong>Robustness maintained</strong> - Sensor noise test: ≤ 20% accuracy drop - Adversarial robustness: ≤ 10% drop under FGSM - Transformer not more brittle than CNNs</p> <p>✅ <strong>MLflow tracking complete</strong> - All Transformer experiments logged - Attention visualizations saved as artifacts - Comparison with ResNet documented</p> <p>✅ <strong>Documentation complete</strong> - README explaining Transformer adaptation to signals - Jupyter notebook: "Training Transformer for Fault Diagnosis" - Attention visualization tutorial - Comparison report: Transformer vs. ResNet vs. Hybrid</p> <h3 id=estimated-effort>Estimated Effort<a class=headerlink href=#estimated-effort title="Permanent link">&para;</a></h3> <p><strong>Time Breakdown:</strong> - Transformer core (5 files): 5 days - <code>signal_transformer.py</code>: 1 day - <code>patch_embedding.py</code>: 0.5 days - <code>positional_encoding.py</code>: 0.5 days - <code>multi_head_attention.py</code>: 1.5 days (complex) - <code>transformer_encoder_layer.py</code>: 1 day - Debugging attention: 0.5 days</p> <ul> <li>Transformer variants (4 files): 3 days</li> <li>ViT adaptation: 1 day</li> <li>Performer (efficient attention): 1 day</li> <li> <p>TFT, Informer: 1 day (optional, lower priority)</p> </li> <li> <p>Training enhancements (3 files): 2 days</p> </li> <li>Warmup scheduler: 0.5 days</li> <li>Transformer-specific augmentation: 1 day</li> <li> <p>Custom trainer: 0.5 days</p> </li> <li> <p>Interpretability (4 files): 4 days</p> </li> <li>Attention visualization: 2 days</li> <li>Attention rollout: 1 day</li> <li>Comparison with Grad-CAM: 0.5 days</li> <li> <p>Interactive dashboard: 0.5 days</p> </li> <li> <p>Hybrid models (2 files): 2 days</p> </li> <li>CNN-Transformer: 1 day</li> <li> <p>Perceiver: 1 day</p> </li> <li> <p>Evaluation (2 files): 2 days</p> </li> <li>Transformer evaluator: 1 day</li> <li> <p>Comparison with ResNet: 1 day</p> </li> <li> <p>Training Transformer models: 3 days</p> </li> <li>Hyperparameter tuning (warmup, lr, layers): 1 day</li> <li>Train standard Transformer: 1 day</li> <li> <p>Train variants (ViT, CNN-Transformer): 1 day</p> </li> <li> <p>Testing (unit, attention, interpretability): 3 days</p> </li> <li>Documentation: 2 days</li> <li>Buffer for debugging: 3 days</li> </ul> <p><strong>Total: ~29 days (1.4 months) for Phase 4</strong></p> <p><strong>Complexity</strong>: ⭐⭐⭐⭐⭐ (Very High) - Transformer architecture is complex (attention mechanism) - Attention visualization requires careful implementation - Warmup schedule is critical (easy to get wrong) - Interpretability comparison (attention vs. Grad-CAM) is research-level</p> <p><strong>Dependencies</strong>: Phase 0 (data), Phase 2 (CNN baseline), Phase 3 (ResNet for comparison and hybrid)</p> <p><strong>Risk</strong>: High - Transformer may not outperform ResNet (attention may not help for signals) - Attention visualization may not be interpretable (random patterns) - Training Transformers is finicky (sensitive to hyperparameters) - Large memory footprint (attention matrices grow quadratically)</p> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024-2026 Syed Abbas Ahmad </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/abbas-ahmad-cowlar/LSTM_PFD target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"annotate": null, "base": "../../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.path", "navigation.top", "navigation.footer", "search.suggest", "search.highlight", "content.tabs.link", "content.code.copy", "content.code.annotate"], "search": "../../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script> <script src=../../../assets/javascripts/bundle.79ae519e.min.js></script> <script src=../../../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>