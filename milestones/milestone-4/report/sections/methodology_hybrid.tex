The complementary nature of CNNs (spatial pattern recognition) and LSTMs (temporal modeling) motivates their integration into hybrid architectures. This section presents our novel configurable hybrid framework that systematically combines CNN feature extraction with LSTM temporal modeling.

\subsubsection{Motivation for Hybrid Architectures}

Bearing vibration signals exhibit both spatial and temporal characteristics that are diagnostically relevant:

\begin{itemize}
    \item \textbf{Spatial Characteristics:} Local impulses, frequency components, signal textures, and fault-specific signatures that CNNs excel at detecting

    \item \textbf{Temporal Characteristics:} Sequential patterns, phase relationships, signal evolution over time, and long-range correlations that LSTMs capture effectively
\end{itemize}

Neither pure CNN nor pure LSTM approaches fully exploit both dimensions. A hybrid architecture can potentially achieve superior performance by:

\begin{enumerate}
    \item Using CNNs as automatic feature extractors, eliminating manual feature engineering
    \item Processing CNN-extracted features through LSTMs to model their temporal evolution
    \item Learning end-to-end from raw signals to fault labels
    \item Leveraging the strengths of both architectures while mitigating their individual limitations
\end{enumerate}

\subsubsection{Configurable Hybrid Framework}

Unlike prior hybrid approaches that tightly couple specific CNN and LSTM architectures, we introduce a \textit{configurable framework} that allows arbitrary combinations. This design philosophy provides several advantages:

\begin{itemize}
    \item \textbf{Flexibility:} Any CNN backbone can be paired with any LSTM type
    \item \textbf{Systematic Exploration:} Enables principled evaluation of different combinations
    \item \textbf{Application-Specific Optimization:} Allows selection based on accuracy, efficiency, or deployment constraints
    \item \textbf{Research Platform:} Facilitates investigation of which architectural choices matter most
\end{itemize}

Our framework supports:
\begin{itemize}
    \item \textbf{7 CNN Backbones:} cnn1d, resnet18, resnet34, resnet50, efficientnet\_b0, efficientnet\_b2, efficientnet\_b4
    \item \textbf{2 LSTM Types:} Vanilla LSTM, Bidirectional LSTM
    \item \textbf{4 Pooling Methods:} Mean, max, last timestep, attention
    \item \textbf{Total Combinations:} 7 × 2 × 4 = 56 possible configurations (with variations in LSTM hidden size and layers, the space exceeds 100 configurations)
\end{itemize}

\subsubsection{Architectural Design}

\paragraph{Overall Architecture Flow}

The hybrid architecture follows a clear hierarchical processing pipeline:

\begin{equation}
\text{Raw Signal} \xrightarrow{\text{CNN}} \text{Feature Sequence} \xrightarrow{\text{LSTM}} \text{Temporal Features} \xrightarrow{\text{Pool}} \text{Fixed Vector} \xrightarrow{\text{FC}} \text{Classification}
\end{equation}

More precisely:

\begin{enumerate}
    \item \textbf{Input:} Raw vibration signal $\mathbf{x} \in \mathbb{R}^{1 \times L}$ where $L=102{,}400$

    \item \textbf{CNN Feature Extraction:} CNN backbone processes the signal through multiple convolutional and pooling layers, producing a feature sequence:
    \begin{equation}
    \mathbf{F} = \text{CNN}(\mathbf{x}) \in \mathbb{R}^{T \times D}
    \end{equation}
    where $T$ is the temporal length after CNN processing, and $D$ is the feature dimension

    \item \textbf{LSTM Temporal Modeling:} The feature sequence is processed by LSTM layers:
    \begin{equation}
    \mathbf{H} = \text{LSTM}(\mathbf{F}) \in \mathbb{R}^{T \times H}
    \end{equation}
    where $H$ is the LSTM hidden size (or $2H$ for BiLSTM)

    \item \textbf{Temporal Aggregation:} The LSTM output sequence is pooled to a fixed-size vector:
    \begin{equation}
    \mathbf{h}_{agg} = \text{Pool}(\mathbf{H}) \in \mathbb{R}^{H}
    \end{equation}

    \item \textbf{Classification:} Fully-connected layers map to class probabilities:
    \begin{equation}
    \mathbf{p} = \text{softmax}(W \mathbf{h}_{agg} + \mathbf{b}) \in \mathbb{R}^{11}
    \end{equation}
\end{enumerate}

\paragraph{CNN Backbone Adaptation}

To use pretrained CNN models as feature extractors, we modify their architecture:

\begin{enumerate}
    \item \textbf{Remove Classification Head:} The final fully-connected layers designed for end-to-end classification are removed

    \item \textbf{Extract Feature Maps:} We extract activations from the final convolutional layer before global pooling, preserving spatial/temporal structure

    \item \textbf{Feature Dimension:} The CNN output dimension $D$ varies by architecture:
    \begin{itemize}
        \item ResNet-18: $D=512$
        \item ResNet-34: $D=512$
        \item ResNet-50: $D=2048$
        \item EfficientNet-B0: $D=1280$
        \item EfficientNet-B2: $D=1408$
        \item EfficientNet-B4: $D=1792$
    \end{itemize}

    \item \textbf{Temporal Length:} After multiple convolutional and pooling operations, the temporal dimension is significantly reduced. For example:
    \begin{itemize}
        \item Input: 102,400 samples
        \item After CNN: Typical $T \in [50, 200]$ depending on architecture and pooling strategies
    \end{itemize}
\end{enumerate}

The CNN backbone can be used in two modes:

\textbf{Frozen Mode:} CNN weights are fixed (not updated during training). This is useful when:
\begin{itemize}
    \item The CNN has been pretrained on similar data
    \item Computational resources are limited
    \item Only LSTM adaptation is desired
    \item Fast training is required
\end{itemize}

\textbf{Fine-tuning Mode:} CNN weights are updated end-to-end with LSTM. This is useful when:
\begin{itemize}
    \item Optimal performance is prioritized over training time
    \item Sufficient training data is available
    \item The task differs significantly from CNN pretraining
\end{itemize}

\paragraph{LSTM Processing Layer}

The LSTM processes the CNN feature sequence:

\begin{lstlisting}[language=Python]
class HybridCNNLSTM(nn.Module):
    def __init__(self, cnn_backbone, lstm_type='bilstm',
                 lstm_hidden_size=256, lstm_num_layers=2,
                 pooling_method='mean'):
        super().__init__()

        # CNN feature extractor
        self.cnn = self._prepare_cnn(cnn_backbone)
        self.cnn_output_dim = self._get_cnn_output_dim()

        # LSTM temporal modeling
        self.lstm = nn.LSTM(
            input_size=self.cnn_output_dim,
            hidden_size=lstm_hidden_size,
            num_layers=lstm_num_layers,
            batch_first=True,
            dropout=0.5 if lstm_num_layers > 1 else 0,
            bidirectional=(lstm_type == 'bilstm')
        )

        # Temporal pooling
        self.pooling = self._create_pooling(pooling_method)

        # Classification head
        lstm_output_dim = lstm_hidden_size * (2 if lstm_type == 'bilstm' else 1)
        self.classifier = nn.Linear(lstm_output_dim, num_classes)

    def forward(self, x):
        # CNN feature extraction: [B, 1, L] -> [B, T, D]
        features = self.cnn(x)

        # LSTM temporal modeling: [B, T, D] -> [B, T, H]
        lstm_out, _ = self.lstm(features)

        # Temporal pooling: [B, T, H] -> [B, H]
        pooled = self.pooling(lstm_out)

        # Classification: [B, H] -> [B, 11]
        logits = self.classifier(pooled)

        return logits
\end{lstlisting}

\paragraph{Temporal Pooling Strategies}

Four pooling methods aggregate LSTM outputs across the temporal dimension:

\textbf{Mean Pooling:}
\begin{equation}
\mathbf{h}_{mean} = \frac{1}{T} \sum_{t=1}^T \mathbf{h}_t
\end{equation}

Advantages: Stable, smooth aggregation of all temporal information. Disadvantages: Equally weights all time steps, potentially diluting important fault signatures.

\textbf{Max Pooling:}
\begin{equation}
\mathbf{h}_{max}[i] = \max_{t \in [1,T]} \mathbf{h}_t[i] \quad \forall i
\end{equation}

Advantages: Emphasizes salient features, robust to noise in irrelevant regions. Disadvantages: May be sensitive to outliers, discards timing information.

\textbf{Last Timestep:}
\begin{equation}
\mathbf{h}_{last} = \mathbf{h}_T
\end{equation}

Advantages: Simple, commonly used in sequence modeling. Disadvantages: Ignores most of the sequence, relies on LSTM's ability to compress all information into final state.

\textbf{Attention Pooling:}
\begin{align}
e_t &= \mathbf{v}^T \tanh(W_a \mathbf{h}_t + \mathbf{b}_a) \\
\alpha_t &= \text{softmax}(e_t) \\
\mathbf{h}_{att} &= \sum_{t=1}^T \alpha_t \mathbf{h}_t
\end{align}

Advantages: Learnable weighting, emphasizes diagnostically relevant time steps, interpretable. Disadvantages: Additional parameters, increased complexity.

\subsubsection{Recommended Configurations}

While our framework supports 56+ configurations, we provide three carefully designed recommendations optimized for different use cases:

\paragraph{Configuration 1: Best Accuracy}

\begin{itemize}
    \item \textbf{CNN Backbone:} ResNet-34
    \item \textbf{LSTM Type:} Bidirectional LSTM
    \item \textbf{LSTM Hidden Size:} 256
    \item \textbf{LSTM Layers:} 2
    \item \textbf{Pooling:} Mean pooling
    \item \textbf{Parameters:} \todo{$\sim$XX.X M}
    \item \textbf{Model Size:} \todo{$\sim$XXX MB}
    \item \textbf{Rationale:} ResNet-34 provides strong feature extraction with proven performance, BiLSTM captures full temporal context, mean pooling provides stable aggregation
\end{itemize}

\paragraph{Configuration 2: Best Efficiency}

\begin{itemize}
    \item \textbf{CNN Backbone:} EfficientNet-B2
    \item \textbf{LSTM Type:} Bidirectional LSTM
    \item \textbf{LSTM Hidden Size:} 256
    \item \textbf{LSTM Layers:} 2
    \item \textbf{Pooling:} Mean pooling
    \item \textbf{Parameters:} \todo{$\sim$XX.X M}
    \item \textbf{Model Size:} \todo{$\sim$XXX MB}
    \item \textbf{Rationale:} EfficientNet-B2 achieves excellent performance with fewer parameters and FLOPs, suitable for resource-constrained deployment
\end{itemize}

\paragraph{Configuration 3: Best Speed}

\begin{itemize}
    \item \textbf{CNN Backbone:} ResNet-18
    \item \textbf{LSTM Type:} Vanilla LSTM (unidirectional)
    \item \textbf{LSTM Hidden Size:} 128
    \item \textbf{LSTM Layers:} 2
    \item \textbf{Pooling:} Last timestep
    \item \textbf{Parameters:} \todo{$\sim$XX.X M}
    \item \textbf{Model Size:} \todo{$\sim$XXX MB}
    \item \textbf{Rationale:} ResNet-18 is the lightest ResNet, unidirectional LSTM halves computation, last timestep pooling is computationally cheapest
\end{itemize}

\subsubsection{Training Methodology}

Hybrid model training follows the same general protocol as CNNs and LSTMs, with specific considerations:

\paragraph{End-to-End Training}

The entire hybrid architecture is trained jointly:

\begin{equation}
\theta^* = \arg\min_\theta \mathcal{L}_{CE}(\text{Hybrid}_\theta(\mathbf{x}), y)
\end{equation}

where $\theta = \{\theta_{CNN}, \theta_{LSTM}, \theta_{classifier}\}$ encompasses all learnable parameters. Joint training allows the CNN to learn features optimized for subsequent LSTM processing, rather than generic features.

\paragraph{Learning Rate Strategy}

Due to different learning dynamics of CNN and LSTM components, we optionally employ discriminative learning rates:

\begin{itemize}
    \item CNN layers: $\eta_{CNN} = 0.0001$ (lower rate if using pretrained weights)
    \item LSTM layers: $\eta_{LSTM} = 0.001$ (higher rate for faster adaptation)
    \item Classifier: $\eta_{classifier} = 0.001$
\end{itemize}

However, for simplicity and unless otherwise noted, we use a uniform learning rate of 0.001 with cosine annealing.

\paragraph{Gradient Flow Considerations}

The hybrid architecture presents unique gradient flow challenges:

\begin{itemize}
    \item Gradients must backpropagate through both LSTM and CNN components
    \item LSTM gradients can be unstable, requiring gradient clipping
    \item Very deep CNNs may compound gradient issues
\end{itemize}

We address these through:
\begin{itemize}
    \item Gradient clipping (threshold = 5.0)
    \item Batch normalization in CNN layers
    \item Careful initialization of LSTM weights
    \item Monitoring gradient norms during training
\end{itemize}

\subsubsection{Computational Complexity Analysis}

The hybrid architecture's computational cost is the sum of CNN and LSTM costs:

\paragraph{Forward Pass Complexity}

\begin{align}
\text{FLOPs}_{total} &= \text{FLOPs}_{CNN} + \text{FLOPs}_{LSTM} \\
&\approx \mathcal{O}(K \cdot D_{in} \cdot D_{out} \cdot L) + \mathcal{O}(4 \cdot H^2 \cdot T \cdot N_{layers})
\end{align}

where $K$ is average kernel size, $D_{in}$, $D_{out}$ are layer dimensions, $H$ is LSTM hidden size, $T$ is sequence length after CNN, and $N_{layers}$ is number of LSTM layers.

\paragraph{Memory Complexity}

\begin{align}
\text{Memory}_{total} &= \text{Memory}_{params} + \text{Memory}_{activations} \\
&\approx \mathcal{O}(|\theta_{CNN}| + |\theta_{LSTM}|) + \mathcal{O}(B \cdot T \cdot H)
\end{align}

where $B$ is batch size. The activation memory for LSTM (required for backpropagation through time) can be substantial for long sequences.

\paragraph{Inference Time}

Inference time is dominated by the slower component. For typical configurations:
\begin{itemize}
    \item CNN forward pass: \todo{$\sim$XX ms}
    \item LSTM forward pass: \todo{$\sim$XX ms}
    \item Total per sample: \todo{$\sim$XX ms}
\end{itemize}

BiLSTM roughly doubles LSTM inference time compared to vanilla LSTM.

\subsubsection{Expected Performance Trade-offs}

We hypothesize the following trade-offs across hybrid configurations:

\begin{table}[h]
\centering
\caption{Expected Hybrid Configuration Trade-offs}
\label{tab:hybrid_tradeoffs}
\small
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Configuration} & \textbf{Accuracy} & \textbf{Speed} & \textbf{Memory} \\
\midrule
Recommended 1 (ResNet34+BiLSTM) & Highest & Moderate & High \\
Recommended 2 (EfficientNet-B2+BiLSTM) & High & Moderate & Moderate \\
Recommended 3 (ResNet18+LSTM) & Moderate-High & Highest & Low \\
\midrule
Custom (ResNet50+BiLSTM) & Very High & Slow & Very High \\
Custom (CNN1D+LSTM) & Moderate & Very Fast & Very Low \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Advantages of Hybrid Approach}

The hybrid architecture offers several theoretical advantages:

\begin{enumerate}
    \item \textbf{Richer Representations:} Combines spatial patterns (CNN) with temporal dynamics (LSTM) for comprehensive feature learning

    \item \textbf{Hierarchical Processing:} CNN pre-processes raw signals into meaningful features, simplifying LSTM's task

    \item \textbf{Multi-Scale Analysis:} CNN captures local patterns, LSTM models global temporal structure

    \item \textbf{Flexibility:} Configurable framework enables optimization for specific requirements

    \item \textbf{Interpretability:} Attention mechanisms and feature visualizations provide insights into fault detection mechanisms
\end{enumerate}

\subsubsection{Implementation}

All hybrid models are implemented in PyTorch 2.0.1:

\begin{lstlisting}[language=Python]
# Example: Create hybrid model
from models import create_model

# Recommended configuration
model = create_model('recommended_1')

# Custom configuration
model = create_model(
    'custom',
    cnn_type='resnet34',
    lstm_type='bilstm',
    lstm_hidden_size=256,
    lstm_num_layers=2,
    pooling_method='mean',
    freeze_cnn=False
)

# Training configuration
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=75
)
criterion = nn.CrossEntropyLoss()
\end{lstlisting}

\subsubsection{Research Questions}

Our hybrid framework enables investigation of several research questions:

\begin{enumerate}
    \item How do different CNN backbones affect hybrid performance when paired with the same LSTM?
    \item What is the performance gap between unidirectional and bidirectional LSTMs in the hybrid context?
    \item Which temporal pooling strategy yields the best performance?
    \item How does freezing CNN weights (transfer learning) impact accuracy and training time?
    \item What is the optimal balance between CNN and LSTM complexity?
\end{enumerate}

These questions are addressed empirically in Section~\ref{sec:results}, where we present comprehensive ablation studies and performance analysis. The configurable nature of our framework makes such systematic investigation tractable.
