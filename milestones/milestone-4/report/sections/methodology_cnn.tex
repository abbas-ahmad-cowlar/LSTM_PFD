Convolutional Neural Networks have emerged as powerful tools for automatic feature learning from raw data. This section details our CNN-based approach for bearing fault diagnosis, covering architectural design, implementation choices, and the rationale behind our methodology.

\subsubsection{CNN Fundamentals for 1D Signals}

Traditional CNNs were designed for 2D image data, but the core principles transfer naturally to 1D time-series data. A 1D convolutional layer applies learnable filters across the temporal dimension:

\begin{equation}
y[n] = \sum_{k=0}^{K-1} w[k] \cdot x[n-k] + b
\end{equation}

where $w$ represents the filter weights, $x$ is the input signal, $K$ is the kernel size, and $b$ is the bias term. Multiple filters are applied in parallel, each learning to detect different patterns in the signal.

The key advantages of CNNs for vibration signal processing include:

\begin{itemize}
    \item \textbf{Local Pattern Detection:} Convolutional filters detect local patterns regardless of their position in the signal (translation invariance), which aligns well with the nature of bearing fault signatures that can occur at any point in the rotation cycle.

    \item \textbf{Hierarchical Feature Learning:} Stacking multiple convolutional layers enables hierarchical feature extraction, with early layers capturing basic patterns (impulses, oscillations) and deeper layers learning complex fault-specific signatures.

    \item \textbf{Parameter Efficiency:} Weight sharing across the temporal dimension dramatically reduces the number of parameters compared to fully-connected networks, preventing overfitting and enabling deeper architectures.

    \item \textbf{Spatial Hierarchy:} Pooling operations progressively reduce the temporal resolution while increasing the receptive field, allowing the network to capture both fine-grained and coarse-grained patterns.
\end{itemize}

\subsubsection{Architectural Variants}

We implement and evaluate 15+ CNN architectures spanning multiple design philosophies. This comprehensive exploration enables systematic assessment of architectural choices and their impact on fault diagnosis performance.

\paragraph{Basic CNN Architectures}

Our basic CNN implementation (CNN1D) serves as a baseline, consisting of:

\begin{itemize}
    \item 4 convolutional blocks, each containing:
    \begin{itemize}
        \item 1D convolutional layer (64, 128, 256, 512 filters respectively)
        \item Batch normalization for training stability
        \item ReLU activation function
        \item Max pooling for downsampling
    \end{itemize}
    \item Global average pooling to aggregate temporal information
    \item Fully-connected classification head
\end{itemize}

This architecture contains approximately 2.3M parameters and provides a straightforward baseline for comparison.

\paragraph{ResNet Family}

Residual Networks\cite{he2016deep} address the degradation problem in deep networks through skip connections. We adapt three ResNet variants for 1D signals:

\textbf{ResNet-18:} The shallowest variant with 18 layers, containing:
\begin{itemize}
    \item Initial conv layer: 64 filters, kernel size 7, stride 2
    \item 4 residual blocks with [2, 2, 2, 2] basic residual units
    \item Filter progression: [64, 128, 256, 512]
    \item Approximately 11M parameters
\end{itemize}

\textbf{ResNet-34:} Medium-depth variant with 34 layers:
\begin{itemize}
    \item Same initial conv layer as ResNet-18
    \item 4 residual blocks with [3, 4, 6, 3] basic residual units
    \item Filter progression: [64, 128, 256, 512]
    \item Approximately 21M parameters
\end{itemize}

\textbf{ResNet-50:} Deepest variant with 50 layers:
\begin{itemize}
    \item Initial conv layer: 64 filters, kernel size 7, stride 2
    \item 4 residual blocks with [3, 4, 6, 3] bottleneck residual units
    \item Bottleneck design uses 1×1 convolutions to reduce/restore dimensionality
    \item Filter progression: [256, 512, 1024, 2048]
    \item Approximately 25M parameters
\end{itemize}

Each residual unit implements the identity mapping:

\begin{equation}
\mathbf{y} = \mathcal{F}(\mathbf{x}, \{W_i\}) + \mathbf{x}
\end{equation}

where $\mathcal{F}$ represents the residual function learned by stacked convolutional layers, and the skip connection adds the input $\mathbf{x}$ directly to the output. This formulation enables gradient flow through hundreds of layers and has proven highly effective for image classification.

\paragraph{EfficientNet Family}

EfficientNets\cite{tan2019efficientnet} achieve state-of-the-art performance through compound scaling that uniformly scales network width, depth, and resolution. We adapt three EfficientNet variants:

\textbf{EfficientNet-B0:} The base model with:
\begin{itemize}
    \item 7 Mobile Inverted Bottleneck (MBConv) blocks
    \item Squeeze-and-Excitation (SE) attention modules
    \item Swish activation function: $f(x) = x \cdot \sigma(\beta x)$
    \item Approximately 5.3M parameters
    \item Optimized width/depth multipliers
\end{itemize}

\textbf{EfficientNet-B2:} Scaled version of B0:
\begin{itemize}
    \item Deeper network (increased depth multiplier)
    \item Wider layers (increased width multiplier)
    \item Approximately 9.2M parameters
    \item Maintains computational efficiency through efficient convolutions
\end{itemize}

\textbf{EfficientNet-B4:} Larger scaled version:
\begin{itemize}
    \item Further increased depth and width
    \item Approximately 19M parameters
    \item Higher capacity for complex pattern learning
\end{itemize}

The MBConv blocks in EfficientNet use depthwise separable convolutions and expansion layers, significantly reducing computational cost while maintaining representational power. SE modules add channel-wise attention, allowing the network to emphasize informative features:

\begin{equation}
\mathbf{y} = \mathbf{x} \odot \sigma(W_2 \delta(W_1 \text{GAP}(\mathbf{x})))
\end{equation}

where GAP denotes global average pooling, $W_1$ and $W_2$ are fully-connected layers, $\delta$ is ReLU activation, $\sigma$ is sigmoid activation, and $\odot$ represents element-wise multiplication.

\subsubsection{Input Representation}

Raw vibration signals are represented as 1D tensors of shape $[B, 1, L]$ where:
\begin{itemize}
    \item $B$: Batch size (typically 32 or 64)
    \item $1$: Number of channels (single-channel accelerometer data)
    \item $L$: Signal length (102,400 samples)
\end{itemize}

This representation treats the vibration signal as a single-channel "image" in the time domain, allowing direct application of convolutional operations without transformation to frequency or time-frequency domains.

\subsubsection{Network Components}

\paragraph{Convolutional Layers}

1D convolutional layers extract local patterns from the input signal. We systematically vary:

\begin{itemize}
    \item \textbf{Kernel Size:} Controls the receptive field of filters. Smaller kernels (3, 5) capture fine-grained patterns, while larger kernels (7, 11) detect broader structures. We primarily use kernel sizes of 3 and 7.

    \item \textbf{Number of Filters:} Determines the representational capacity. We follow the common practice of progressively increasing filter counts in deeper layers: 64 → 128 → 256 → 512.

    \item \textbf{Stride:} Controls the downsampling rate. We use stride 2 in specific layers to reduce temporal resolution and computational cost.

    \item \textbf{Padding:} We apply "same" padding to preserve temporal dimensions and "valid" padding for dimension reduction.
\end{itemize}

\paragraph{Activation Functions}

Non-linear activation functions enable networks to learn complex decision boundaries:

\begin{itemize}
    \item \textbf{ReLU} (Rectified Linear Unit): $f(x) = \max(0, x)$

    Used in ResNet and basic CNN architectures for its simplicity and effective gradient propagation.

    \item \textbf{Swish}: $f(x) = x \cdot \sigma(\beta x)$

    Used in EfficientNet architectures, providing smooth non-linearity and improved performance.
\end{itemize}

\paragraph{Normalization}

Batch normalization normalizes layer inputs across the mini-batch:

\begin{equation}
\hat{x} = \frac{x - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}
\end{equation}

where $\mu_{\mathcal{B}}$ and $\sigma_{\mathcal{B}}^2$ are the batch mean and variance. Learnable scale ($\gamma$) and shift ($\beta$) parameters allow the network to adapt the normalization:

\begin{equation}
y = \gamma \hat{x} + \beta
\end{equation}

Batch normalization accelerates training, enables higher learning rates, and provides regularization through mini-batch statistics.

\paragraph{Pooling Operations}

Pooling layers reduce spatial dimensions and provide translation invariance:

\begin{itemize}
    \item \textbf{Max Pooling:} $y[n] = \max_{k \in \mathcal{N}(n)} x[k]$

    Selects the maximum value within each pooling window, emphasizing salient features.

    \item \textbf{Average Pooling:} $y[n] = \frac{1}{|\mathcal{N}(n)|} \sum_{k \in \mathcal{N}(n)} x[k]$

    Computes the mean within each window, providing smoother downsampling.

    \item \textbf{Global Average Pooling (GAP):} Aggregates the entire temporal dimension into a single value per channel, reducing parameters in the classification head.
\end{itemize}

\paragraph{Classification Head}

After convolutional feature extraction, we employ fully-connected layers for classification:

\begin{itemize}
    \item Global average pooling reduces feature maps to a fixed-size vector
    \item Optional dropout layer (p=0.5) for regularization
    \item Fully-connected layer mapping to 11 output classes
    \item Softmax activation for probability distribution:
    \begin{equation}
    p_i = \frac{e^{z_i}}{\sum_{j=1}^{11} e^{z_j}}
    \end{equation}
    where $z_i$ is the logit for class $i$.
\end{itemize}

\subsubsection{Training Methodology}

\paragraph{Loss Function}

We employ cross-entropy loss for multi-class classification:

\begin{equation}
\mathcal{L}_{CE} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(\hat{y}_{i,c})
\end{equation}

where $N$ is the batch size, $C=11$ is the number of classes, $y_{i,c}$ is the one-hot encoded ground truth, and $\hat{y}_{i,c}$ is the predicted probability for sample $i$ and class $c$.

\paragraph{Optimization}

We use the Adam optimizer\cite{kingma2014adam} with adaptive learning rates:

\begin{align}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \\
\theta_t &= \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align}

where $g_t$ is the gradient at step $t$, $m_t$ and $v_t$ are first and second moment estimates, and $\beta_1=0.9$, $\beta_2=0.999$ are decay rates. Initial learning rate $\alpha=0.001$.

\paragraph{Learning Rate Scheduling}

We employ cosine annealing\cite{loshchilov2016sgdr} for smooth learning rate decay:

\begin{equation}
\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{T_{cur}}{T_{max}}\pi\right)\right)
\end{equation}

where $\eta_{max}=0.001$, $\eta_{min}=0.00001$, $T_{cur}$ is the current epoch, and $T_{max}$ is the total number of epochs.

\paragraph{Regularization Techniques}

\begin{enumerate}
    \item \textbf{Dropout:} Randomly drops units during training with probability $p=0.5$ in fully-connected layers, preventing co-adaptation of features.

    \item \textbf{Weight Decay:} L2 regularization penalizes large weights:
    \begin{equation}
    \mathcal{L}_{total} = \mathcal{L}_{CE} + \lambda \sum_{i} w_i^2
    \end{equation}
    with $\lambda=0.0001$.

    \item \textbf{Data Augmentation:} Applied stochastically during training as described in Section~\ref{sec:dataset}.

    \item \textbf{Early Stopping:} Training halts if validation loss doesn't improve for 15 consecutive epochs, preventing overfitting.
\end{enumerate}

\paragraph{Mixed Precision Training}

We leverage automatic mixed precision (AMP) training with FP16 arithmetic for computational efficiency:

\begin{itemize}
    \item Forward passes computed in FP16 for speed
    \item Backward passes use FP32 for numerical stability
    \item Gradient scaling prevents underflow
    \item Achieves ~2× speedup on modern GPUs with minimal accuracy impact
\end{itemize}

\subsubsection{Implementation Details}

All CNN models are implemented in PyTorch 2.0.1 using the following configuration:

\begin{lstlisting}[language=Python]
# Model Configuration
num_classes = 11
input_shape = (1, 102400)
batch_size = 32

# Training Configuration
num_epochs = 75
initial_lr = 0.001
optimizer = 'adam'
scheduler = 'cosine'
weight_decay = 0.0001

# Regularization
dropout_prob = 0.5
data_augmentation = True
early_stopping_patience = 15

# Hardware
device = 'cuda'  # NVIDIA GPU
mixed_precision = True
num_workers = 4  # Data loading threads
\end{lstlisting}

\subsubsection{Model Selection and Evaluation}

During training, we monitor both training and validation metrics. The model checkpoint with the highest validation accuracy is selected as the final model for test set evaluation. This prevents selecting models that overfit to the training data.

For each architecture, we report:
\begin{itemize}
    \item Overall classification accuracy
    \item Per-class precision, recall, and F1-score
    \item Confusion matrix
    \item Number of parameters
    \item Model size (MB)
    \item Inference time per sample
    \item Training time per epoch
\end{itemize}

\subsubsection{Rationale for Multiple Architectures}

The inclusion of 15+ CNN variants serves several purposes:

\begin{enumerate}
    \item \textbf{Baseline Establishment:} Simple CNNs provide performance baselines for comparison with more sophisticated architectures.

    \item \textbf{Architectural Comparison:} Systematic evaluation reveals which architectural innovations (residual connections, efficient scaling, attention mechanisms) provide the most benefit for bearing fault diagnosis.

    \item \textbf{Computational Trade-offs:} Different architectures offer various accuracy-efficiency trade-offs, allowing selection based on deployment constraints (edge devices vs. cloud servers).

    \item \textbf{Robustness Assessment:} Evaluating multiple architectures provides confidence that results are not architecture-specific artifacts.

    \item \textbf{Transfer Learning Potential:} Understanding which architectures excel enables informed choices for transfer learning to new bearing types or fault categories.
\end{enumerate}

\subsubsection{Expected Outcomes}

Based on literature and preliminary experiments, we hypothesize that:

\begin{itemize}
    \item ResNet architectures will achieve high accuracy due to their depth and skip connections
    \item EfficientNet models will provide strong performance with fewer parameters
    \item Deeper networks (ResNet-50, EfficientNet-B4) may risk overfitting given the dataset size
    \item Basic CNNs will establish competitive baselines, demonstrating the power of automatic feature learning
\end{itemize}

The actual performance results are presented in Section~\ref{sec:results}, where we provide comprehensive empirical validation of these hypotheses.
