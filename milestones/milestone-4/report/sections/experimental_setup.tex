This section details the experimental protocols, hardware infrastructure, software environment, evaluation metrics, and statistical analysis procedures used throughout this study.

\subsection{Hardware and Software Environment}

All experiments were conducted on consistent hardware to ensure fair comparison:

\paragraph{Hardware Configuration}
\begin{itemize}
    \item \textbf{GPU:} \todo{NVIDIA GPU model, XX GB VRAM}
    \item \textbf{CPU:} \todo{XX cores, XX GHz}
    \item \textbf{RAM:} \todo{XX GB}
    \item \textbf{Storage:} \todo{SSD configuration}
\end{itemize}

\paragraph{Software Stack}
\begin{itemize}
    \item \textbf{Operating System:} \todo{Linux distribution and version}
    \item \textbf{Python:} 3.9.12
    \item \textbf{PyTorch:} 2.0.1 with CUDA 11.8
    \item \textbf{cuDNN:} 8.7.0
    \item \textbf{NumPy:} 1.24.3
    \item \textbf{SciPy:} 1.10.1 (for .mat file loading)
    \item \textbf{scikit-learn:} 1.3.0 (for metrics)
    \item \textbf{Matplotlib:} 3.7.1 (for visualization)
\end{itemize}

\subsection{Training Configuration}

Unless otherwise specified, all models use the following training hyperparameters:

\paragraph{Common Training Parameters}
\begin{itemize}
    \item \textbf{Number of Epochs:} 75
    \item \textbf{Batch Size:} 32 for CNNs and Hybrids, 16-32 for LSTMs
    \item \textbf{Initial Learning Rate:} 0.001
    \item \textbf{Optimizer:} Adam with $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=10^{-8}$
    \item \textbf{Weight Decay:} 0.0001
    \item \textbf{Learning Rate Schedule:} Cosine annealing
    \item \textbf{Gradient Clipping:} Enabled for LSTMs and Hybrids (threshold = 5.0)
    \item \textbf{Mixed Precision:} Enabled (FP16) for computational efficiency
    \item \textbf{Random Seed:} 42 (for reproducibility)
\end{itemize}

\paragraph{Data Loading}
\begin{itemize}
    \item \textbf{Num Workers:} 4 (parallel data loading threads)
    \item \textbf{Pin Memory:} Enabled (for faster GPU transfer)
    \item \textbf{Shuffle:} Enabled for training set, disabled for validation/test sets
\end{itemize}

\paragraph{Regularization}
\begin{itemize}
    \item \textbf{Dropout:} 0.5 in fully-connected layers
    \item \textbf{Data Augmentation:} Applied to training set only
    \begin{itemize}
        \item Additive Gaussian noise: $\sigma \sim \mathcal{U}(0.01, 0.05)$
        \item Amplitude scaling: $\beta \sim \mathcal{U}(0.8, 1.2)$
        \item Temporal shifting: Random circular shift
        \item Application probability: 50\% per augmentation
    \end{itemize}
    \item \textbf{Early Stopping:} Patience of 15 epochs based on validation loss
\end{itemize}

\subsection{Model-Specific Configurations}

\paragraph{CNN Models}
\begin{itemize}
    \item Input shape: $[B, 1, 102400]$
    \item All 15+ architectures trained with identical hyperparameters
    \item Batch size: 32
    \item Global average pooling before classification
\end{itemize}

\paragraph{LSTM Models}
\begin{itemize}
    \item Input sequence length: 10,240 (downsampled by factor of 10)
    \item Hidden size: 128 or 256
    \item Number of layers: 2
    \item Bidirectional: True for BiLSTM, False for Vanilla LSTM
    \item Batch size: 32 (reduced to 16 if memory constrained)
\end{itemize}

\paragraph{Hybrid Models}
\begin{itemize}
    \item CNN backbone: Variable (resnet18, resnet34, resnet50, efficientnet variants)
    \item LSTM type: BiLSTM or Vanilla LSTM
    \item LSTM hidden size: 256 for recommended configs, variable for custom
    \item LSTM layers: 2
    \item Temporal pooling: Mean, max, last, or attention
    \item Batch size: 32
\end{itemize}

\subsection{Evaluation Metrics}

We employ comprehensive metrics to assess model performance from multiple perspectives:

\subsubsection{Classification Metrics}

\paragraph{Overall Accuracy}

The primary metric, measuring the percentage of correctly classified samples:

\begin{equation}
\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Samples}}
\end{equation}

\paragraph{Per-Class Metrics}

For each fault class $c$, we compute:

\textbf{Precision:} The fraction of predicted class $c$ samples that are truly class $c$:
\begin{equation}
\text{Precision}_c = \frac{TP_c}{TP_c + FP_c}
\end{equation}

\textbf{Recall (Sensitivity):} The fraction of true class $c$ samples correctly identified:
\begin{equation}
\text{Recall}_c = \frac{TP_c}{TP_c + FN_c}
\end{equation}

\textbf{F1-Score:} The harmonic mean of precision and recall:
\begin{equation}
F1_c = 2 \cdot \frac{\text{Precision}_c \cdot \text{Recall}_c}{\text{Precision}_c + \text{Recall}_c}
\end{equation}

where $TP_c$, $FP_c$, and $FN_c$ denote true positives, false positives, and false negatives for class $c$.

\paragraph{Macro-Averaged Metrics}

We compute macro-averaged precision, recall, and F1-score by averaging per-class metrics:

\begin{equation}
\text{Macro-F1} = \frac{1}{C} \sum_{c=1}^{C} F1_c
\end{equation}

where $C=11$ is the number of classes. Macro-averaging treats all classes equally, providing insight into performance across both common and rare faults.

\paragraph{Confusion Matrix}

We construct $11 \times 11$ confusion matrices showing the distribution of predictions for each true class. The confusion matrix enables identification of:
\begin{itemize}
    \item Frequently confused fault pairs
    \item Classes with high/low classification accuracy
    \item Systematic misclassification patterns
\end{itemize}

\subsubsection{Computational Metrics}

\paragraph{Model Size}

Total number of learnable parameters and model size in megabytes:
\begin{equation}
\text{Size (MB)} = \frac{\text{Number of Parameters} \times 4 \text{ bytes}}{1024^2}
\end{equation}

assuming 32-bit floating point storage.

\paragraph{Training Time}

\begin{itemize}
    \item Time per epoch (seconds)
    \item Total training time until convergence
    \item GPU memory usage during training
\end{itemize}

\paragraph{Inference Time}

\begin{itemize}
    \item Time per sample (milliseconds)
    \item Throughput (samples per second)
    \item Measured on both GPU and CPU for deployment considerations
\end{itemize}

\paragraph{FLOPs}

Floating point operations per forward pass, indicating computational complexity.

\subsubsection{Robustness Metrics}

\paragraph{Performance Under Noise}

To assess robustness, we evaluate models on test data corrupted with varying levels of additive Gaussian noise:

\begin{equation}
\mathbf{x}_{noisy} = \mathbf{x} + \mathcal{N}(0, \sigma^2)
\end{equation}

with $\sigma \in \{0.01, 0.05, 0.1, 0.2\}$, reporting accuracy degradation at each noise level.

\paragraph{Cross-Loading Generalization}

We evaluate models trained at one load condition on test data from different load conditions to assess generalization capability.

\subsection{Statistical Analysis}

To ensure robust conclusions, we employ rigorous statistical methodology:

\paragraph{Multiple Training Runs}

Each model configuration is trained \todo{3-5} times with different random seeds to account for training variability. We report:
\begin{itemize}
    \item Mean performance across runs
    \item Standard deviation
    \item Best and worst performance
\end{itemize}

\paragraph{Statistical Significance Testing}

When comparing two approaches, we apply paired t-tests with significance level $\alpha=0.05$. For multiple comparisons, we apply Bonferroni correction to control family-wise error rate.

\paragraph{Confidence Intervals}

We report 95\% confidence intervals for key metrics:
\begin{equation}
CI_{95} = \bar{x} \pm 1.96 \frac{s}{\sqrt{n}}
\end{equation}

where $\bar{x}$ is the sample mean, $s$ is the standard deviation, and $n$ is the number of runs.

\subsection{Experimental Protocols}

\subsubsection{Protocol 1: Individual Approach Evaluation}

For each of the three approaches (CNN, LSTM, Hybrid):

\begin{enumerate}
    \item Train all architectural variants with identical hyperparameters
    \item Evaluate on validation set to select best model per architecture
    \item Perform final evaluation on held-out test set
    \item Report comprehensive metrics (accuracy, precision, recall, F1, confusion matrix)
    \item Analyze failure cases and misclassification patterns
\end{enumerate}

\subsubsection{Protocol 2: Cross-Approach Comparison}

To fairly compare CNN, LSTM, and Hybrid approaches:

\begin{enumerate}
    \item Select best-performing architecture from each approach
    \item Ensure identical data splits, preprocessing, and evaluation procedures
    \item Train multiple times with different random seeds
    \item Compute mean and standard deviation of performance metrics
    \item Perform statistical significance testing
    \item Analyze computational trade-offs (accuracy vs. speed vs. model size)
\end{enumerate}

\subsubsection{Protocol 3: Ablation Studies}

For hybrid models, we conduct systematic ablation studies:

\begin{enumerate}
    \item \textbf{CNN Backbone Ablation:} Fix LSTM configuration, vary CNN backbone (resnet18 vs. resnet34 vs. efficientnet-b2, etc.)

    \item \textbf{LSTM Type Ablation:} Fix CNN backbone, compare Vanilla LSTM vs. BiLSTM

    \item \textbf{Pooling Method Ablation:} Fix CNN and LSTM, compare pooling strategies (mean vs. max vs. last vs. attention)

    \item \textbf{LSTM Depth Ablation:} Vary number of LSTM layers (1 vs. 2 vs. 3)

    \item \textbf{LSTM Hidden Size Ablation:} Vary hidden dimension (64 vs. 128 vs. 256 vs. 512)

    \item \textbf{Transfer Learning Ablation:} Compare frozen CNN vs. fine-tuned CNN
\end{enumerate}

Each ablation isolates one architectural choice while keeping others constant, enabling principled analysis of design decisions.

\subsection{Reproducibility Measures}

To ensure reproducibility, we implement:

\begin{enumerate}
    \item \textbf{Fixed Random Seeds:} All random number generators (Python, NumPy, PyTorch, cuDNN) are seeded with value 42

    \item \textbf{Deterministic Operations:} PyTorch deterministic mode enabled where possible:
    \begin{lstlisting}[language=Python]
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
    \end{lstlisting}

    \item \textbf{Version Pinning:} All package versions explicitly specified in requirements.txt

    \item \textbf{Complete Code Availability:} Full implementations, training scripts, and evaluation tools provided

    \item \textbf{Comprehensive Documentation:} Detailed README files, usage examples, and API documentation

    \item \textbf{Checkpoint Availability:} Trained model weights available for download (subject to storage constraints)
\end{enumerate}

\subsection{Validation Strategy}

We employ a three-level validation strategy:

\paragraph{Level 1: Validation Set During Training}

\begin{itemize}
    \item Monitor validation loss and accuracy after each epoch
    \item Early stopping based on validation performance
    \item Learning rate scheduling informed by validation plateau
\end{itemize}

\paragraph{Level 2: Validation Set for Model Selection}

\begin{itemize}
    \item Compare different architectures on validation set
    \item Select best hyperparameters using validation performance
    \item Ablation studies use validation set for intermediate comparisons
\end{itemize}

\paragraph{Level 3: Test Set for Final Evaluation}

\begin{itemize}
    \item Held-out test set used only once for final performance reporting
    \item No model selection or hyperparameter tuning based on test set
    \item Provides unbiased estimate of generalization performance
\end{itemize}

This hierarchical validation prevents information leakage and overfitting to evaluation data.

\subsection{Experimental Timeline}

The complete experimental program comprises:

\begin{itemize}
    \item \textbf{Milestone 1 (CNN):} \todo{XX} architectures $\times$ \todo{3-5} runs $\times$ \todo{XX} hours/run $\approx$ \todo{XX} hours total
    \item \textbf{Milestone 2 (LSTM):} \todo{XX} configurations $\times$ \todo{3-5} runs $\times$ \todo{XX} hours/run $\approx$ \todo{XX} hours total
    \item \textbf{Milestone 3 (Hybrid):} \todo{XX} configurations $\times$ \todo{3-5} runs $\times$ \todo{XX} hours/run $\approx$ \todo{XX} hours total
    \item \textbf{Ablation Studies:} \todo{XX} experiments $\times$ \todo{XX} hours/experiment $\approx$ \todo{XX} hours total
    \item \textbf{Total Compute Time:} \todo{XXX-XXX} GPU hours
\end{itemize}

All experiments were conducted \todo{over a period of XX weeks}.

\subsection{Ethical Considerations}

This research uses publicly available benchmark data (CWRU dataset) and does not involve human subjects, personal data, or sensitive information. The work aims to improve industrial safety through better fault diagnosis, presenting no ethical concerns. All software tools used are either open-source or properly licensed.
