This section presents comprehensive experimental results for all three approaches: CNN-based (Milestone 1), LSTM-based (Milestone 2), and Hybrid CNN-LSTM (Milestone 3). We provide quantitative performance metrics, qualitative analysis, computational comparisons, and ablation studies.

\subsection{Overall Performance Comparison}

Table~\ref{tab:overall_results} summarizes the performance of the best model from each approach on the test set.

\begin{table}[h]
\centering
\caption{Overall Performance Comparison Across Three Approaches}
\label{tab:overall_results}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Approach} & \textbf{Best Model} & \textbf{Accuracy (\%)} & \textbf{Macro-F1} & \textbf{Parameters} \\
\midrule
CNN (M1) & \todo{ResNet-XX} & \todo{XX.XX $\pm$ X.XX} & \todo{X.XXX} & \todo{XX.XM} \\
LSTM (M2) & \todo{BiLSTM/Vanilla} & \todo{XX.XX $\pm$ X.XX} & \todo{X.XXX} & \todo{XX.XM} \\
Hybrid (M3) & \todo{Config-X} & \todo{XX.XX $\pm$ X.XX} & \todo{X.XXX} & \todo{XX.XM} \\
\bottomrule
\end{tabular}
\end{table}

\figplaceholder{Figure: Bar chart comparing overall accuracy of the three approaches with error bars showing standard deviation across multiple runs}

\paragraph{Key Findings:}

\todo{[After completing experiments, this paragraph will summarize which approach achieved the best performance, the magnitude of improvement, and whether differences are statistically significant. For example: "The hybrid approach achieved XX.XX\% accuracy, outperforming pure CNN (XX.XX\%) by X.X percentage points and pure LSTM (XX.XX\%) by X.X percentage points. Statistical testing with paired t-test revealed that the hybrid improvement is significant (p < 0.05)."]}

\subsection{Milestone 1: CNN-Based Approach Results}

\subsubsection{Architecture Comparison}

We evaluated 15+ CNN architectures spanning multiple design families. Table~\ref{tab:cnn_results} presents the complete results.

\begin{table}[h]
\centering
\caption{CNN Architecture Performance Comparison}
\label{tab:cnn_results}
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Architecture} & \textbf{Params (M)} & \textbf{Acc (\%)} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} & \textbf{Inf. Time (ms)} \\
\midrule
\multicolumn{7}{c}{\textit{Basic CNNs}} \\
\midrule
CNN-1D & \todo{X.X} & \todo{XX.XX} & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX.X} \\
\midrule
\multicolumn{7}{c}{\textit{ResNet Family}} \\
\midrule
ResNet-18 & \todo{XX.X} & \todo{XX.XX} & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX.X} \\
ResNet-34 & \todo{XX.X} & \todo{XX.XX} & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX.X} \\
ResNet-50 & \todo{XX.X} & \todo{XX.XX} & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX.X} \\
\midrule
\multicolumn{7}{c}{\textit{EfficientNet Family}} \\
\midrule
EfficientNet-B0 & \todo{X.X} & \todo{XX.XX} & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX.X} \\
EfficientNet-B2 & \todo{X.X} & \todo{XX.XX} & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX.X} \\
EfficientNet-B4 & \todo{XX.X} & \todo{XX.XX} & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX.X} \\
\bottomrule
\end{tabular}
\end{table}

\figplaceholder{Figure: Scatter plot showing accuracy vs. parameters for all CNN architectures, highlighting the accuracy-efficiency trade-off}

\figplaceholder{Figure: Confusion matrix for the best-performing CNN architecture (11x11 heatmap)}

\paragraph{Analysis:}

\todo{[This paragraph will analyze: (1) Which CNN family performed best (ResNet vs. EfficientNet vs. basic CNNs), (2) Whether deeper networks outperformed shallower ones, (3) Efficiency considerations (parameters vs. accuracy), (4) Specific architectural features that contributed to performance (skip connections, efficient scaling, etc.). Example: "ResNet-34 achieved the highest accuracy at XX.XX\%, demonstrating that skip connections are beneficial for this task. Interestingly, ResNet-50 showed only marginal improvement (XX.XX\%) despite having significantly more parameters, suggesting potential overfitting on this dataset size. EfficientNet-B2 provided an excellent balance with XX.XX\% accuracy and only X.XM parameters."]}

\subsubsection{Per-Class Performance}

Table~\ref{tab:cnn_per_class} shows per-class metrics for the best CNN model.

\begin{table}[h]
\centering
\caption{Per-Class Performance of Best CNN Model}
\label{tab:cnn_per_class}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Fault Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\midrule
Healthy & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX} \\
Misalignment & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX} \\
Imbalance & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX} \\
Bearing Clearance & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX} \\
Lubrication Issue & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX} \\
Cavitation & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX} \\
Wear & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX} \\
Oil Whirl & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX} \\
Mixed Fault 1 & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX} \\
Mixed Fault 2 & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX} \\
Mixed Fault 3 & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XX} \\
\midrule
\textbf{Macro Avg} & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XXX} \\
\textbf{Weighted Avg} & \todo{X.XXX} & \todo{X.XXX} & \todo{X.XXX} & \todo{XXX} \\
\bottomrule
\end{tabular}
\end{table}

\figplaceholder{Figure: Bar chart showing F1-scores for each fault class, identifying strengths and weaknesses}

\paragraph{Observations:}

\todo{[This will discuss: (1) Which fault types are easiest/hardest to classify, (2) Performance on single vs. mixed faults, (3) Common confusion patterns, (4) Potential reasons for class-specific performance differences. Example: "Healthy bearing classification achieved perfect or near-perfect precision (X.XXX) as expected, given its distinct signature. Mixed Fault 2 (bearing clearance + lubrication) proved most challenging with F1-score of X.XXX, likely due to overlapping vibration patterns from multiple degradation mechanisms."]}

\subsubsection{Training Dynamics}

\figplaceholder{Figure: Training and validation curves (loss and accuracy) for best CNN model over 75 epochs}

\figplaceholder{Figure: Learning rate schedule showing cosine annealing decay}

\paragraph{Training Characteristics:}

\todo{[Analysis of: (1) Convergence speed (epochs to reach 95\% of final performance), (2) Overfitting behavior (training vs. validation gap), (3) Effect of early stopping, (4) Stability across different random seeds. Example: "The best CNN model converged rapidly, reaching XX\% validation accuracy within XX epochs. A small train-validation gap of X.X\% suggests limited overfitting. Early stopping triggered at epoch XX, preventing further overfitting observed in preliminary experiments without this regularization."]}

\subsection{Milestone 2: LSTM-Based Approach Results}

\subsubsection{LSTM Variant Comparison}

Table~\ref{tab:lstm_results} compares Vanilla LSTM and BiLSTM configurations.

\begin{table}[h]
\centering
\caption{LSTM Architecture Performance Comparison}
\label{tab:lstm_results}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Architecture} & \textbf{Hidden} & \textbf{Params (M)} & \textbf{Acc (\%)} & \textbf{F1} & \textbf{Train Time (h)} & \textbf{Inf. (ms)} \\
\midrule
Vanilla LSTM & 128 & \todo{X.X} & \todo{XX.XX} & \todo{X.XXX} & \todo{X.X} & \todo{XX.X} \\
Vanilla LSTM & 256 & \todo{X.X} & \todo{XX.XX} & \todo{X.XXX} & \todo{X.X} & \todo{XX.X} \\
\midrule
BiLSTM & 128 & \todo{X.X} & \todo{XX.XX} & \todo{X.XXX} & \todo{X.X} & \todo{XX.X} \\
BiLSTM & 256 & \todo{X.X} & \todo{XX.XX} & \todo{X.XXX} & \todo{X.X} & \todo{XX.X} \\
\bottomrule
\end{tabular}
\end{table}

\figplaceholder{Figure: Bar chart comparing Vanilla LSTM vs. BiLSTM performance}

\paragraph{Findings:}

\todo{[Analysis will cover: (1) Performance gap between unidirectional and bidirectional LSTMs, (2) Impact of hidden size on accuracy, (3) Computational trade-offs, (4) Whether bidirectional processing justifies the doubled cost. Example: "BiLSTM with hidden size 256 achieved the best accuracy at XX.XX\%, outperforming Vanilla LSTM by X.X percentage points. This improvement comes at the cost of doubled parameters and ~2× longer inference time. For offline analysis where computational budget permits, BiLSTM is clearly superior."]}

\subsubsection{Comparison with CNN Approach}

Direct comparison between best LSTM and best CNN:

\begin{table}[h]
\centering
\caption{CNN vs. LSTM: Head-to-Head Comparison}
\label{tab:cnn_vs_lstm}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Metric} & \textbf{Best CNN} & \textbf{Best LSTM} & \textbf{Difference} & \textbf{p-value} & \textbf{Significant?} \\
\midrule
Accuracy (\%) & \todo{XX.XX $\pm$ X.XX} & \todo{XX.XX $\pm$ X.XX} & \todo{$\pm$X.XX} & \todo{X.XXX} & \todo{Yes/No} \\
Macro-F1 & \todo{X.XXX $\pm$ X.XXX} & \todo{X.XXX $\pm$ X.XXX} & \todo{$\pm$X.XXX} & \todo{X.XXX} & \todo{Yes/No} \\
Params (M) & \todo{XX.X} & \todo{X.X} & \todo{$-$XX.X} & - & - \\
Inf. Time (ms) & \todo{XX.X} & \todo{XX.X} & \todo{$\pm$XX.X} & - & - \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Discussion:}

\todo{[This will interpret: (1) Which approach achieved better accuracy, (2) Magnitude and statistical significance of differences, (3) Advantages and disadvantages of each approach, (4) Scenarios where one might be preferred over the other. Example: "CNN achieved marginally higher accuracy (XX.XX\%) compared to LSTM (XX.XX\%), but the difference of X.X percentage points is not statistically significant (p=X.XXX). However, CNNs demonstrate significantly faster inference (XX.X ms vs. XX.X ms), making them more suitable for real-time applications. The LSTM's advantage in explicit temporal modeling did not translate to substantial performance gains on this dataset, possibly because CNNs capture sufficient temporal information through their receptive fields."]}

\subsubsection{LSTM Attention Analysis}

For LSTM models with attention mechanisms, we visualize learned attention weights:

\figplaceholder{Figure: Attention weight heatmaps for different fault classes, showing which temporal regions the LSTM focuses on for classification}

\paragraph{Interpretability:}

\todo{[Discussion of: (1) Which temporal regions receive high attention for each fault type, (2) Whether attention patterns align with domain knowledge about fault signatures, (3) Differences in attention patterns across fault classes. Example: "Attention visualizations reveal that the LSTM focuses heavily on the first XXX time steps for bearing clearance faults, corresponding to the initial impact in each rotation cycle. In contrast, lubrication faults show more distributed attention across the entire sequence, reflecting their gradual, non-impulsive nature."]}

\subsection{Milestone 3: Hybrid CNN-LSTM Results}

\subsubsection{Recommended Configuration Performance}

Table~\ref{tab:hybrid_recommended} presents results for the three recommended hybrid configurations.

\begin{table}[h]
\centering
\caption{Performance of Recommended Hybrid Configurations}
\label{tab:hybrid_recommended}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Configuration} & \textbf{Architecture} & \textbf{Params (M)} & \textbf{Acc (\%)} & \textbf{F1} & \textbf{Inf. (ms)} \\
\midrule
Recommended 1 & ResNet34 + BiLSTM & \todo{XX.X} & \todo{XX.XX $\pm$ X.XX} & \todo{X.XXX} & \todo{XX.X} \\
Recommended 2 & EfficientNet-B2 + BiLSTM & \todo{XX.X} & \todo{XX.XX $\pm$ X.XX} & \todo{X.XXX} & \todo{XX.X} \\
Recommended 3 & ResNet18 + LSTM & \todo{XX.X} & \todo{XX.XX $\pm$ X.XX} & \todo{X.XXX} & \todo{XX.X} \\
\bottomrule
\end{tabular}
\end{table}

\figplaceholder{Figure: Comparison of the three recommended configurations across multiple metrics (accuracy, speed, model size)}

\paragraph{Configuration Analysis:}

\todo{[Will discuss: (1) Which configuration achieved best accuracy, (2) Trade-offs between configurations, (3) Which to choose for different deployment scenarios. Example: "Recommended 1 (ResNet34+BiLSTM) achieved the highest accuracy at XX.XX\%, validating its design for maximum performance. Recommended 2 provided competitive accuracy (XX.XX\%) with XX\% fewer parameters, making it ideal for resource-constrained deployment. Recommended 3 offered the fastest inference (XX.X ms) suitable for real-time monitoring, with acceptable accuracy of XX.XX\%."]}

\subsubsection{Ablation Studies}

To understand which design choices matter most, we conducted systematic ablation studies.

\paragraph{Ablation 1: CNN Backbone Selection}

Fixing LSTM configuration (BiLSTM, hidden=256, 2 layers, mean pooling), we varied the CNN backbone:

\begin{table}[h]
\centering
\caption{Ablation Study: CNN Backbone Selection}
\label{tab:ablation_cnn}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{CNN Backbone} & \textbf{CNN Params (M)} & \textbf{Total Params (M)} & \textbf{Accuracy (\%)} & \textbf{$\Delta$ from Best} \\
\midrule
CNN-1D & \todo{X.X} & \todo{X.X} & \todo{XX.XX} & \todo{-X.XX} \\
ResNet-18 & \todo{XX.X} & \todo{XX.X} & \todo{XX.XX} & \todo{-X.XX} \\
ResNet-34 & \todo{XX.X} & \todo{XX.X} & \todo{XX.XX} & \todo{0.00} \\
ResNet-50 & \todo{XX.X} & \todo{XX.X} & \todo{XX.XX} & \todo{-X.XX} \\
EfficientNet-B0 & \todo{X.X} & \todo{X.X} & \todo{XX.XX} & \todo{-X.XX} \\
EfficientNet-B2 & \todo{X.X} & \todo{X.X} & \todo{XX.XX} & \todo{-X.XX} \\
EfficientNet-B4 & \todo{XX.X} & \todo{XX.X} & \todo{XX.XX} & \todo{-X.XX} \\
\bottomrule
\end{tabular}
\end{table}

\figplaceholder{Figure: Line plot showing accuracy vs. CNN backbone complexity}

\todo{[Discussion will explain which CNN backbones work best with LSTM, whether deeper CNNs provide better features, and the accuracy-efficiency trade-off. Example: "ResNet-34 emerged as the optimal CNN backbone with XX.XX\% accuracy. Surprisingly, the deeper ResNet-50 underperformed (XX.XX\%), suggesting that its higher-capacity features may be over-specialized for end-to-end classification. EfficientNet-B2 provided strong performance (XX.XX\%) with excellent parameter efficiency."]}

\paragraph{Ablation 2: LSTM Type}

Fixing CNN backbone (ResNet-34), we compared LSTM types:

\begin{table}[h]
\centering
\caption{Ablation Study: LSTM Type Selection}
\label{tab:ablation_lstm_type}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{LSTM Type} & \textbf{Hidden Size} & \textbf{Params (M)} & \textbf{Accuracy (\%)} & \textbf{Inference (ms)} \\
\midrule
Vanilla LSTM & 128 & \todo{XX.X} & \todo{XX.XX} & \todo{XX.X} \\
Vanilla LSTM & 256 & \todo{XX.X} & \todo{XX.XX} & \todo{XX.X} \\
BiLSTM & 128 & \todo{XX.X} & \todo{XX.XX} & \todo{XX.X} \\
BiLSTM & 256 & \todo{XX.X} & \todo{XX.XX} & \todo{XX.X} \\
\bottomrule
\end{tabular}
\end{table}

\todo{[Will discuss the accuracy gain from bidirectional processing and larger hidden sizes, and whether the computational cost is justified.]}

\paragraph{Ablation 3: Temporal Pooling Strategy}

Fixing architecture (ResNet-34 + BiLSTM-256), we compared pooling methods:

\begin{table}[h]
\centering
\caption{Ablation Study: Temporal Pooling Methods}
\label{tab:ablation_pooling}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Pooling Method} & \textbf{Additional Params} & \textbf{Accuracy (\%)} & \textbf{$\Delta$ from Mean} \\
\midrule
Mean & 0 & \todo{XX.XX} & \todo{0.00} \\
Max & 0 & \todo{XX.XX} & \todo{$\pm$X.XX} \\
Last Timestep & 0 & \todo{XX.XX} & \todo{$\pm$X.XX} \\
Attention & \todo{XXK} & \todo{XX.XX} & \todo{$\pm$X.XX} \\
\bottomrule
\end{tabular}
\end{table}

\figplaceholder{Figure: Visualization of attention weights learned by the attention pooling mechanism}

\todo{[Discussion of which pooling method works best and whether attention's added complexity yields meaningful gains. Example: "Mean pooling achieved XX.XX\% accuracy, matching or exceeding other strategies. Attention pooling marginally improved performance (XX.XX\%), but the X.XX\% gain may not justify the added complexity for most applications. Interestingly, last timestep pooling significantly underperformed (XX.XX\%), suggesting that compressing all information into the final LSTM state is suboptimal for this task."]}

\paragraph{Ablation 4: CNN Freezing (Transfer Learning)}

Comparing frozen CNN (weights fixed) vs. end-to-end fine-tuning:

\begin{table}[h]
\centering
\caption{Ablation Study: CNN Freezing Strategy}
\label{tab:ablation_freezing}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Training Strategy} & \textbf{Accuracy (\%)} & \textbf{Training Time (h)} & \textbf{GPU Memory (GB)} \\
\midrule
Frozen CNN & \todo{XX.XX} & \todo{X.X} & \todo{X.X} \\
Fine-tuned CNN & \todo{XX.XX} & \todo{X.X} & \todo{X.X} \\
\midrule
Improvement & \todo{$+$X.XX} & \todo{$-$X.X} & \todo{$+$X.X} \\
\bottomrule
\end{tabular}
\end{table}

\todo{[Will analyze the accuracy-speed trade-off of transfer learning. Example: "End-to-end fine-tuning improved accuracy by X.XX percentage points (XX.XX\% vs. XX.XX\%), demonstrating that task-specific CNN adaptation is beneficial. However, this came at the cost of X.X× longer training time. For rapid prototyping or limited computational budgets, frozen CNN provides a reasonable compromise."]}

\subsubsection{Hybrid vs. Individual Approaches}

Comprehensive comparison of the best model from each milestone:

\begin{table}[h]
\centering
\caption{Comprehensive Three-Way Comparison}
\label{tab:three_way}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Approach} & \textbf{Model} & \textbf{Acc (\%)} & \textbf{F1} & \textbf{Params (M)} & \textbf{Size (MB)} & \textbf{Inf. (ms)} \\
\midrule
CNN & \todo{ResNet-XX} & \todo{XX.XX} & \todo{X.XXX} & \todo{XX.X} & \todo{XXX} & \todo{XX.X} \\
LSTM & \todo{BiLSTM-XXX} & \todo{XX.XX} & \todo{X.XXX} & \todo{X.X} & \todo{XX} & \todo{XX.X} \\
Hybrid & \todo{ResNetXX+BiLSTM} & \todo{XX.XX} & \todo{X.XXX} & \todo{XX.X} & \todo{XXX} & \todo{XX.X} \\
\bottomrule
\end{tabular}
\end{table}

\figplaceholder{Figure: Radar chart comparing the three approaches across multiple dimensions: accuracy, speed, model size, training time, robustness}

\paragraph{Comparative Analysis:}

\todo{[Comprehensive discussion addressing: (1) Did the hybrid approach achieve the best accuracy? (2) What is the performance gain over individual approaches? (3) Is the gain worth the added complexity? (4) When would you choose each approach? Example detailed analysis: "The hybrid approach achieved XX.XX\% accuracy, surpassing CNN (XX.XX\%) by X.X points and LSTM (XX.XX\%) by X.X points. Statistical testing confirms significance (p<0.05 for both comparisons). This validates our hypothesis that combining spatial and temporal modeling yields superior performance. However, the hybrid model is significantly larger (XX.XM parameters vs. XX.XM for CNN and X.XM for LSTM) and slower (XX.X ms vs. XX.X ms for CNN). For deployment scenarios prioritizing accuracy over computational constraints, the hybrid approach is clearly superior. For real-time edge deployment, pure CNN offers the best accuracy-efficiency trade-off."]}

\subsection{Robustness Analysis}

\subsubsection{Performance Under Noise}

We evaluated all three approaches under varying noise levels:

\begin{table}[h]
\centering
\caption{Accuracy Degradation Under Additive Noise}
\label{tab:noise_robustness}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Approach} & \textbf{Clean} & \textbf{$\sigma$=0.01} & \textbf{$\sigma$=0.05} & \textbf{$\sigma$=0.1} \\
\midrule
CNN & \todo{XX.XX\%} & \todo{XX.XX\%} & \todo{XX.XX\%} & \todo{XX.XX\%} \\
LSTM & \todo{XX.XX\%} & \todo{XX.XX\%} & \todo{XX.XX\%} & \todo{XX.XX\%} \\
Hybrid & \todo{XX.XX\%} & \todo{XX.XX\%} & \todo{XX.XX\%} & \todo{XX.XX\%} \\
\bottomrule
\end{tabular}
\end{table}

\figplaceholder{Figure: Line plot showing accuracy vs. noise level for all three approaches}

\paragraph{Robustness Findings:}

\todo{[Will discuss: (1) Which approach is most robust to noise, (2) Rate of performance degradation, (3) Potential reasons for different robustness levels. Example: "All approaches demonstrated graceful degradation with increasing noise. At moderate noise ($\sigma$=0.05), accuracy dropped by X.X\%, X.X\%, and X.X\% for CNN, LSTM, and Hybrid respectively. The hybrid approach showed superior robustness, likely due to its hierarchical processing where CNN features provide some noise filtering before LSTM processing."]}

\subsubsection{Confusion Analysis}

Detailed analysis of common misclassification patterns:

\figplaceholder{Figure: Confusion matrices for CNN, LSTM, and Hybrid approaches side-by-side for comparison}

\paragraph{Misclassification Patterns:}

\todo{[Analysis of: (1) Which fault pairs are most frequently confused, (2) Whether confusion patterns differ across approaches, (3) Physical interpretation of confusions. Example: "All approaches occasionally confused Mixed Fault 2 (bearing clearance + lubrication) with Lubrication Issue alone, achieving only XX-XX\% precision for this class. This is physically reasonable as lubrication faults likely dominate the vibration signature in the mixed fault. Interestingly, the hybrid approach showed fewer confusions between Misalignment and Imbalance (X% error rate vs. X% for CNN), suggesting its temporal modeling better distinguishes these faults with different time-domain characteristics."]}

\subsection{Computational Efficiency Analysis}

\subsubsection{Training Efficiency}

\begin{table}[h]
\centering
\caption{Training Time and Resource Requirements}
\label{tab:training_efficiency}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Approach} & \textbf{Time/Epoch (s)} & \textbf{Total Time (h)} & \textbf{GPU Mem (GB)} & \textbf{Convergence Epoch} \\
\midrule
CNN (ResNet-34) & \todo{XXX} & \todo{X.X} & \todo{X.X} & \todo{XX} \\
LSTM (BiLSTM-256) & \todo{XXX} & \todo{X.X} & \todo{X.X} & \todo{XX} \\
Hybrid (ResNet34+BiLSTM) & \todo{XXX} & \todo{X.X} & \todo{X.X} & \todo{XX} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Training Characteristics:}

\todo{[Discussion of training efficiency, convergence speed, and resource requirements across approaches.]}

\subsubsection{Inference Efficiency}

\begin{table}[h]
\centering
\caption{Inference Performance on Different Hardware}
\label{tab:inference_efficiency}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Approach} & \textbf{GPU (ms/sample)} & \textbf{CPU (ms/sample)} & \textbf{Throughput (samples/s)} & \textbf{Model Size (MB)} \\
\midrule
CNN & \todo{XX.X} & \todo{XXX} & \todo{XXX} & \todo{XXX} \\
LSTM & \todo{XX.X} & \todo{XXX} & \todo{XXX} & \todo{XX} \\
Hybrid & \todo{XX.X} & \todo{XXX} & \todo{XXX} & \todo{XXX} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Deployment Considerations:}

\todo{[Will discuss: (1) Real-time capability of each approach, (2) Suitability for edge vs. cloud deployment, (3) Recommendations based on deployment constraints. Example: "For real-time monitoring requiring sub-100ms latency, CNN is the only viable option (XX.X ms inference time). LSTM and hybrid approaches exceed this threshold (XX.X ms and XX.X ms respectively) but are suitable for offline batch analysis. On CPU hardware, all approaches become significantly slower, with hybrid requiring XXX ms per sample."]}

\subsection{Summary of Key Results}

\begin{enumerate}
    \item \textbf{Overall Performance:} \todo{[Hybrid/CNN/LSTM] achieved the highest test accuracy of XX.XX\%, establishing new state-of-the-art on this dataset.}

    \item \textbf{CNN Findings:} \todo{[ResNet-XX/EfficientNet-XX] provided the best accuracy-efficiency trade-off among CNN architectures with XX.XX\% accuracy and XX.XM parameters.}

    \item \textbf{LSTM Findings:} \todo{BiLSTM outperformed vanilla LSTM by X.X percentage points, justifying bidirectional processing for offline analysis.}

    \item \textbf{Hybrid Advantages:} \todo{The hybrid approach improved accuracy by X.X points over pure CNN and X.X points over pure LSTM, demonstrating the benefit of combining spatial and temporal modeling.}

    \item \textbf{Ablation Insights:} \todo{CNN backbone choice proved most critical (X.X\% accuracy range), followed by LSTM type (X.X\% difference) and pooling method (X.X\% difference).}

    \item \textbf{Robustness:} \todo{All approaches maintained >XX\% accuracy under moderate noise ($\sigma$=0.05), with hybrid showing best robustness.}

    \item \textbf{Efficiency:} \todo{CNN offered fastest inference (XX.X ms), while hybrid sacrificed speed for accuracy. LSTM provided a middle ground.}
\end{enumerate}

These results comprehensively validate our three-milestone approach and demonstrate that each methodology offers distinct advantages depending on application requirements.
